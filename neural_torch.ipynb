{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, random, time\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "from utils import save_value, load_value, load_env_keys\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '/scratch/juanmoo1'\n",
    "EMA_dump_path = os.path.join(data_path, './jsons/EMA_dump.json')\n",
    "EMA_xmls_path = os.path.join(data_path, './xmls/')\n",
    "EMA_annotations_path = os.path.join(data_path, './bayer/VendorEMAforMIT/annotations.xlsx')\n",
    "\n",
    "pickle_dumps_path = os.path.join(data_path, './pickle_dumps/')\n",
    "checkpoint_path = os.path.join(pickle_dumps_path, 'checkpoint.pickle')\n",
    "\n",
    "models_path = os.path.join(data_path, './models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_documents = load_value('labeled_raw_documents', path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, spacy\n",
    "\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "MAX_CHARS = 20000\n",
    "\n",
    "def tokenizer(comment):\n",
    "    comment = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(comment))\n",
    "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
    "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
    "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
    "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
    "    if (len(comment) > MAX_CHARS):\n",
    "        comment = comment[:MAX_CHARS]\n",
    "    return [x.text for x in NLP.tokenizer(comment) if x.text != \" \"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_paragraph_length = 5\n",
    "vocab = set()\n",
    "label_vocab = set()\n",
    "tag_vocab = set()\n",
    "\n",
    "processed_docs = dict()\n",
    "\n",
    "for doc_name in labeled_raw_documents:\n",
    "    doc = labeled_raw_documents[doc_name]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    tags = []\n",
    "    \n",
    "    for text, label, tag in zip(doc['paragraphs'], doc['labels'], doc['tags']):\n",
    "        words = tokenizer(text.lower())\n",
    "        vocab |= set(words)\n",
    "        tag_vocab.add(tag.lower())\n",
    "\n",
    "        \n",
    "        if len(words) >= minimum_paragraph_length and tag in ['head', 'p']:\n",
    "            vocab |= set(words)\n",
    "            label_vocab.add(label.lower())\n",
    "\n",
    "            texts.append(words)\n",
    "            labels.append(label.lower())\n",
    "            tags.append(tag.lower())\n",
    "    \n",
    "    processed_docs[doc_name] = {\n",
    "        'texts': texts,\n",
    "        'labels': labels,\n",
    "        'tags': tags\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Match text with the previous header\n",
    "for doc_name in processed_docs:\n",
    "    texts = processed_docs[doc_name]['texts']\n",
    "    tags = processed_docs[doc_name]['tags']\n",
    "    \n",
    "    # First Header\n",
    "    header1 = [-1] * len(texts)    \n",
    "    last_header = 0\n",
    "    \n",
    "    while(last_header < len(tags) and tags[last_header] != 'head'):\n",
    "        last_header += 1\n",
    "        \n",
    "    i = last_header + 1\n",
    "    \n",
    "    while i < len(tags):\n",
    "        header1[i] = last_header\n",
    "        if tags[i] == 'head':\n",
    "            last_header = i\n",
    "        i += 1\n",
    "    \n",
    "    # Second Header\n",
    "    header2 = [-1] * len(header1)\n",
    "    last_header = 0\n",
    "    while(last_header < len(tags) and header1[last_header] == -1):\n",
    "        last_header += 1\n",
    "        \n",
    "    i = last_header + 1\n",
    "    while i < len(header2):\n",
    "        header2[i] = header1[last_header]\n",
    "        if header1[i] != -1:\n",
    "            last_header = i\n",
    "        i += 1\n",
    "        \n",
    "    first_header = [texts[i] if i != -1 else [] for i in header1]\n",
    "    second_header = [texts[i] if i != -1 else [] for i in header2]\n",
    "    \n",
    "    processed_docs[doc_name]['first_header'] = first_header\n",
    "    processed_docs[doc_name]['second_header'] = second_header\n",
    "\n",
    "save_value('processed_documents_2', processed_docs, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'significant findings - fertility': 0, 'significant findings - lactation': 1, 'populations - geriatric': 2, 'significant findings - renal impairment': 3, 'other': 4, 'warning': 5, 'populations - adult': 6, 'populations - adolescent': 7, 'significant findings - hepatic impairment': 8, 'contraindication': 9, 'populations - paediatric': 10, 'significant findings - pregnancy': 11}\n"
     ]
    }
   ],
   "source": [
    "lab2index = {l:i for i, l in enumerate(label_vocab)}\n",
    "print(lab2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google's Pretrained word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(models_path, 'GoogleNews-vectors-negative300.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pretrained embeading params\n",
    "google_weights = word2vec.vectors\n",
    "google_index2word = word2vec.index2word\n",
    "google_word2index = word2vec.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_vocab = set(word2vec.vocab)\n",
    "unknown_vocab = list(vocab - google_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown Portion: 0.36820633384040935\n"
     ]
    }
   ],
   "source": [
    "# Portion of unknown words\n",
    "print('Unknown Portion:', len(unknown_vocab)/len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random embedding for unknown words\n",
    "unknown = nn.Embedding(len(unknown_vocab), 300)\n",
    "unknown_weights = unknown.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint Embeddings\n",
    "weights = torch.tensor(np.vstack([google_weights, unknown_weights]), dtype=torch.float)\n",
    "index2word = google_index2word + unknown_vocab\n",
    "word2index = {w:i for i, w in enumerate(index2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Torch embedding\n",
    "embedding = nn.EmbeddingBag.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([0]), torch.tensor([0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([[word2index['their']], [word2index['our']]])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP w/ Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        # Input to first fc layer is 3 * embedding size to account for prev two headers\n",
    "        self.fc = nn.Linear(embedding.embedding_dim * 3, 500)\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        # Embedding doesn't get randomly initialized\n",
    "#         self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, data, offsets):\n",
    "        emb_text = self.embedding(data[0], offsets[0]) # text\n",
    "        emb_h1 = self.embedding(data[1], offsets[1]) # first header\n",
    "        emb_h2 = self.embedding(data[2], offsets[2]) # second header\n",
    "        embedded = torch.cat([emb_text, emb_h1, emb_h2], dim=1)\n",
    "        \n",
    "        out = self.fc(embedded)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_vocab)\n",
    "model = Classifier(embedding, num_classes)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([3005325, 300]),\n",
       " torch.Size([500, 900]),\n",
       " torch.Size([500]),\n",
       " torch.Size([12, 500]),\n",
       " torch.Size([12])]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(processed_docs)\n",
    "random.shuffle(names)\n",
    "\n",
    "i = int(.8 * len(names))\n",
    "train_docs = names[:i]\n",
    "test_docs = names[i:]\n",
    "\n",
    "# (texts, header1, header2), label\n",
    "\n",
    "train_data = []\n",
    "for doc_name in train_docs:\n",
    "    doc = processed_docs[doc_name]\n",
    "    texts = doc['texts']\n",
    "    header1s = doc['first_header']\n",
    "    header2s = doc['second_header']\n",
    "    labels = doc['labels']\n",
    "    \n",
    "    for t, h1, h2, l in zip(texts, header1s, header2s, labels):\n",
    "        data = (t, h1, h2)\n",
    "        train_data.append((data, l))\n",
    "        \n",
    "test_data = []\n",
    "for doc_name in test_docs:\n",
    "    doc = processed_docs[doc_name]\n",
    "    texts = doc['texts']\n",
    "    header1s = doc['first_header']\n",
    "    header2s = doc['second_header']\n",
    "    labels = doc['labels']\n",
    "    \n",
    "    for t, h1, h2, l in zip(texts, header1s, header2s, labels):\n",
    "        data = (t, h1, h2)\n",
    "        test_data.append((data, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch): #batch <- [((text, header_1, header_2), label), ...]\n",
    "    data, labels = zip(*batch)\n",
    "    \n",
    "    label = torch.tensor([lab2index[entry[1]] for entry in batch])\n",
    "    texts, h1s, h2s = zip(*data)\n",
    "    \n",
    "    texts = [ torch.tensor([word2index[w] for w in t], dtype=torch.long) for t in texts ]\n",
    "    h1s = [ torch.tensor([word2index[w] for w in h], dtype=torch.long) for h in h1s ]\n",
    "    h2s = [ torch.tensor([word2index[w] for w in h], dtype=torch.long) for h in h2s ]\n",
    "    \n",
    "    text_offsets = [0] + [len(entry) for entry in texts[:-1]]\n",
    "    text_offsets = torch.tensor(text_offsets).cumsum(dim=0)\n",
    "    \n",
    "    h1_offsets = [0] + [len(entry) for entry in h1s[:-1]]\n",
    "    h1_offsets = torch.tensor(h1_offsets).cumsum(dim=0)\n",
    "    \n",
    "    h2_offsets = [0] + [len(entry) for entry in h2s[:-1]]\n",
    "    h2_offsets = torch.tensor(h2_offsets).cumsum(dim=0)\n",
    "    \n",
    "    texts = torch.cat(texts)\n",
    "    h1s = torch.cat(h1s)\n",
    "    h2s = torch.cat(h2s)\n",
    "    \n",
    "    data = [texts, h1s, h2s]\n",
    "    offsets = [text_offsets, h1_offsets, h2_offsets]\n",
    "    \n",
    "    return label, data, offsets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Training Function\n",
    "def train_func(data):\n",
    "    \n",
    "    # Metrics\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    # Mini-batch training\n",
    "    random.shuffle(data)\n",
    "    for i in range(len(data)//BATCH_SIZE):\n",
    "        batch = data[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        labels, d, offsets = process_batch(batch)\n",
    "        output = model(d, offsets)\n",
    "        loss = criterion(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss/len(data), train_acc/len(data)\n",
    "\n",
    "# Testing Function\n",
    "def test_func(data):\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    all_pred = torch.tensor([], dtype=torch.long)\n",
    "    all_lab = torch.tensor([], dtype=torch.long)\n",
    "    \n",
    "    for i in range(len(data)//BATCH_SIZE):\n",
    "        batch = data[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "        labels, d, offsets = process_batch(batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(d, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(1)\n",
    "            all_pred = torch.cat([all_pred, pred])\n",
    "            all_lab = torch.cat([all_lab, labels])\n",
    "            test_acc += (pred == labels).sum().item()\n",
    "    \n",
    "    return all_pred, all_lab, test_loss/len(data), test_acc/len(data)\n",
    "\n",
    "# Precision & Recall from confusion matrix\n",
    "def get_metrics(cm, other_index = None):\n",
    "    # Diagonal elemetns were correctly classified\n",
    "    diagonal = cm.diagonal()\n",
    "    \n",
    "    # Input class Counts\n",
    "    class_sum = cm.sum(axis=1)\n",
    "    \n",
    "    # Predicted class counts\n",
    "    pred_sum = cm.sum(axis=0)\n",
    "    \n",
    "    # Per-class performance w/ no-examples -> 0 perf\n",
    "    precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
    "    recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n",
    "    \n",
    "    # Frequency Weighted Performance\n",
    "    c_freq = cm.sum(axis=1)/cm.sum()\n",
    "    pres = c_freq * precision\n",
    "    rec = c_freq * recall\n",
    "\n",
    "    # Remove 'other' Category\n",
    "    if other_index is not None:\n",
    "        c_freq = np.hstack([c_freq[0:other_index], c_freq[other_index + 1:]])\n",
    "        pres = np.hstack([pres[0:other_index], pres[other_index + 1:]])\n",
    "        rec = np.hstack([rec[0:other_index], rec[other_index + 1:]])\n",
    "    \n",
    "    return pres.sum()/c_freq.sum(), rec.sum()/c_freq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "min_valid_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss()#.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0098(train)\t|\tAcc: 94.3%(train)\n",
      "Test:\tLoss: 0.0248(valid)\t|\tAcc: 93.6%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 2  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0097(train)\t|\tAcc: 94.3%(train)\n",
      "Test:\tLoss: 0.0242(valid)\t|\tAcc: 94.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 3  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0096(train)\t|\tAcc: 94.5%(train)\n",
      "Test:\tLoss: 0.0245(valid)\t|\tAcc: 94.5%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 4  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0095(train)\t|\tAcc: 94.4%(train)\n",
      "Test:\tLoss: 0.0244(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 5  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0094(train)\t|\tAcc: 94.5%(train)\n",
      "Test:\tLoss: 0.0246(valid)\t|\tAcc: 94.4%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 6  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0093(train)\t|\tAcc: 94.5%(train)\n",
      "Test:\tLoss: 0.0249(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 7  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0093(train)\t|\tAcc: 94.6%(train)\n",
      "Test:\tLoss: 0.0251(valid)\t|\tAcc: 94.6%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 8  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0092(train)\t|\tAcc: 94.6%(train)\n",
      "Test:\tLoss: 0.0253(valid)\t|\tAcc: 94.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 9  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0092(train)\t|\tAcc: 94.7%(train)\n",
      "Test:\tLoss: 0.0256(valid)\t|\tAcc: 94.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 10  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0091(train)\t|\tAcc: 94.6%(train)\n",
      "Test:\tLoss: 0.0253(valid)\t|\tAcc: 94.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 11  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0091(train)\t|\tAcc: 94.7%(train)\n",
      "Test:\tLoss: 0.0255(valid)\t|\tAcc: 94.4%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 12  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0091(train)\t|\tAcc: 94.6%(train)\n",
      "Test:\tLoss: 0.0259(valid)\t|\tAcc: 93.8%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 13  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0090(train)\t|\tAcc: 94.7%(train)\n",
      "Test:\tLoss: 0.0258(valid)\t|\tAcc: 94.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 14  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0090(train)\t|\tAcc: 94.7%(train)\n",
      "Test:\tLoss: 0.0259(valid)\t|\tAcc: 93.6%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 15  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0090(train)\t|\tAcc: 94.7%(train)\n",
      "Test:\tLoss: 0.0259(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 16  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0089(train)\t|\tAcc: 94.8%(train)\n",
      "Test:\tLoss: 0.0257(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 17  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0089(train)\t|\tAcc: 94.8%(train)\n",
      "Test:\tLoss: 0.0259(valid)\t|\tAcc: 94.4%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 18  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0089(train)\t|\tAcc: 94.8%(train)\n",
      "Test:\tLoss: 0.0259(valid)\t|\tAcc: 94.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 19  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0089(train)\t|\tAcc: 94.8%(train)\n",
      "Test:\tLoss: 0.0259(valid)\t|\tAcc: 94.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 20  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0089(train)\t|\tAcc: 94.8%(train)\n",
      "Test:\tLoss: 0.0258(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 21  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.8%(train)\n",
      "Test:\tLoss: 0.0261(valid)\t|\tAcc: 94.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 22  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.8%(train)\n",
      "Test:\tLoss: 0.0259(valid)\t|\tAcc: 94.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 23  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.9%(train)\n",
      "Test:\tLoss: 0.0261(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 24  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.9%(train)\n",
      "Test:\tLoss: 0.0261(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 25  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.9%(train)\n",
      "Test:\tLoss: 0.0260(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 26  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.8%(train)\n",
      "Test:\tLoss: 0.0261(valid)\t|\tAcc: 94.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 27  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.9%(train)\n",
      "Test:\tLoss: 0.0260(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 28  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.9%(train)\n",
      "Test:\tLoss: 0.0261(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 29  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.9%(train)\n",
      "Test:\tLoss: 0.0261(valid)\t|\tAcc: 94.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 30  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0088(train)\t|\tAcc: 94.9%(train)\n",
      "Test:\tLoss: 0.0261(valid)\t|\tAcc: 94.2%(valid)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(train_data)\n",
    "    pred, lab, valid_loss, valid_acc = test_func(test_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'Train:\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'Test:\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "    print('-' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    1    0    0    0    0    0    0]\n",
      " [   0 2665   52    0    0    3    0    6]\n",
      " [   0   65   72    0    0    0    0    0]\n",
      " [   0    1    0    0    0    0    0    0]\n",
      " [   0    1    0    0    0    0    0    0]\n",
      " [   0   14    2    0    0    4    0    0]\n",
      " [   0    4    0    0    0    0    0    0]\n",
      " [   0   21    1    0    0    0    0   32]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred = pred.numpy()\n",
    "lab = lab.numpy()\n",
    "\n",
    "cm = confusion_matrix(lab, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorder based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2665   65   21   14    1    1    1    4]\n",
      " [  52   72    1    2    0    0    0    0]\n",
      " [   6    0   32    0    0    0    0    0]\n",
      " [   3    0    0    4    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "class_count = dict(zip(*np.unique(pred, return_counts=True)))\n",
    "rev_map = sorted(class_count, key=class_count.get, reverse=True)\n",
    "missing = list(set(range(12)) - set(rev_map))\n",
    "rev_map.extend(missing)\n",
    "\n",
    "mapping = list(range(12))\n",
    "for i, e in enumerate(rev_map):\n",
    "    mapping[e] = i\n",
    "\n",
    "new_pred = [mapping[e] for e in pred]\n",
    "new_lab = [mapping[e] for e in lab]\n",
    "\n",
    "cm = confusion_matrix(new_pred, new_lab)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9419157608695652\n",
      "Recall: 0.9513022068741\n",
      "Without 'other':\n",
      "Precision: 0.627906976744186\n",
      "Recall: 0.5271107213137429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/j/juanmoo1/bin/miniconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "precision, recall = get_metrics(cm, other_index=None)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "print('Without \\'other\\':')\n",
    "precision, recall = get_metrics(cm, other_index=0)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP, Embeddings, No header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        # Input to first fc layer is 3 * embedding size to account for prev two headers\n",
    "        self.fc = nn.Linear(embedding.embedding_dim, 500)\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        # Embedding doesn't get randomly initialized\n",
    "#         self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, data, offsets):\n",
    "        embedded = self.embedding(data[0], offsets[0]) # text\n",
    "        out = self.fc(embedded)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_vocab)\n",
    "model = Classifier(embedding, num_classes)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "min_valid_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss()#.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0051(train)\t|\tAcc: 97.1%(train)\n",
      "Test:\tLoss: 0.0223(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 2  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0050(train)\t|\tAcc: 97.2%(train)\n",
      "Test:\tLoss: 0.0233(valid)\t|\tAcc: 92.7%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 3  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0049(train)\t|\tAcc: 97.2%(train)\n",
      "Test:\tLoss: 0.0223(valid)\t|\tAcc: 93.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 4  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0048(train)\t|\tAcc: 97.3%(train)\n",
      "Test:\tLoss: 0.0222(valid)\t|\tAcc: 93.5%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 5  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0048(train)\t|\tAcc: 97.4%(train)\n",
      "Test:\tLoss: 0.0232(valid)\t|\tAcc: 93.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 6  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0047(train)\t|\tAcc: 97.3%(train)\n",
      "Test:\tLoss: 0.0223(valid)\t|\tAcc: 93.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 7  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0046(train)\t|\tAcc: 97.5%(train)\n",
      "Test:\tLoss: 0.0224(valid)\t|\tAcc: 93.3%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 8  | time in 0 minutes, 4 seconds\n",
      "Train:\tLoss: 0.0046(train)\t|\tAcc: 97.5%(train)\n",
      "Test:\tLoss: 0.0230(valid)\t|\tAcc: 93.4%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 9  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0045(train)\t|\tAcc: 97.5%(train)\n",
      "Test:\tLoss: 0.0236(valid)\t|\tAcc: 93.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 10  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0045(train)\t|\tAcc: 97.5%(train)\n",
      "Test:\tLoss: 0.0229(valid)\t|\tAcc: 93.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 11  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0044(train)\t|\tAcc: 97.6%(train)\n",
      "Test:\tLoss: 0.0233(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 12  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0044(train)\t|\tAcc: 97.5%(train)\n",
      "Test:\tLoss: 0.0233(valid)\t|\tAcc: 93.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 13  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0043(train)\t|\tAcc: 97.6%(train)\n",
      "Test:\tLoss: 0.0235(valid)\t|\tAcc: 93.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 14  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0043(train)\t|\tAcc: 97.6%(train)\n",
      "Test:\tLoss: 0.0239(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 15  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0043(train)\t|\tAcc: 97.6%(train)\n",
      "Test:\tLoss: 0.0238(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 16  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0043(train)\t|\tAcc: 97.6%(train)\n",
      "Test:\tLoss: 0.0235(valid)\t|\tAcc: 93.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 17  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0043(train)\t|\tAcc: 97.6%(train)\n",
      "Test:\tLoss: 0.0238(valid)\t|\tAcc: 93.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 18  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0042(train)\t|\tAcc: 97.6%(train)\n",
      "Test:\tLoss: 0.0236(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 19  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0042(train)\t|\tAcc: 97.6%(train)\n",
      "Test:\tLoss: 0.0237(valid)\t|\tAcc: 93.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 20  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0042(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0240(valid)\t|\tAcc: 93.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 21  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0042(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0237(valid)\t|\tAcc: 93.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 22  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0042(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0236(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 23  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0042(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0236(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 24  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0042(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0240(valid)\t|\tAcc: 93.2%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 25  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0041(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0241(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 26  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0041(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0240(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 27  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0041(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0238(valid)\t|\tAcc: 93.0%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 28  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0041(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0238(valid)\t|\tAcc: 92.9%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 29  | time in 0 minutes, 2 seconds\n",
      "Train:\tLoss: 0.0041(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0241(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n",
      "Epoch: 30  | time in 0 minutes, 3 seconds\n",
      "Train:\tLoss: 0.0041(train)\t|\tAcc: 97.7%(train)\n",
      "Test:\tLoss: 0.0240(valid)\t|\tAcc: 93.1%(valid)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(train_data)\n",
    "    pred, lab, valid_loss, valid_acc = test_func(test_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'Train:\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'Test:\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "    print('-' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2699  121   41    8    2    1    0    1    1]\n",
      " [  12   16    0    0    0    0    0    0    0]\n",
      " [   6    0   13    0    0    0    0    0    0]\n",
      " [   4    0    0   12    0    0    0    0    0]\n",
      " [   4    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    2    0    0    0    0]\n",
      " [   1    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred = pred.numpy()\n",
    "lab = lab.numpy()\n",
    "class_count = dict(zip(*np.unique(pred, return_counts=True)))\n",
    "rev_map = sorted(class_count, key=class_count.get, reverse=True)\n",
    "missing = list(set(range(12)) - set(rev_map))\n",
    "rev_map.extend(missing)\n",
    "\n",
    "mapping = list(range(12))\n",
    "for i, e in enumerate(rev_map):\n",
    "    mapping[e] = i\n",
    "\n",
    "new_pred = [mapping[e] for e in pred]\n",
    "new_lab = [mapping[e] for e in lab]\n",
    "\n",
    "cm = confusion_matrix(new_pred, new_lab)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9307065217391305\n",
      "Recall: 0.9724790295473974\n",
      "\n",
      "Without 'other':\n",
      "Precision: 0.5857142857142856\n",
      "Recall: 0.24920210095392573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/j/juanmoo1/bin/miniconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in true_divide\n",
      "/afs/csail.mit.edu/u/j/juanmoo1/bin/miniconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:64: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "precision, recall = get_metrics(cm, other_index=None)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "print('\\nWithout \\'other\\':')\n",
    "precision, recall = get_metrics(cm, other_index=0)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
