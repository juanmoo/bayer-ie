{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "import utils, json\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from linear_model import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '/scratch/juanmoo1/bayer'\n",
    "\n",
    "EMA_annotations_path = os.path.join(data_path, 'VendorEMAforMIT/newLabels/annotations.xlsx')\n",
    "EMA_old_annotations_path = os.path.join(data_path, 'VendorEMAforMIT/annotations.xlsx')\n",
    "\n",
    "EMA_old_parsed_path = os.path.join(data_path, './VendorEMAforMIT/Labels/parsed.json')\n",
    "EMA_parsed_path = os.path.join(data_path, './VendorEMAforMIT/newLabels/parsed.json')\n",
    "\n",
    "pickle_dumps_path = os.path.join(data_path, 'pickle_dumps/')\n",
    "checkpoint_path = os.path.join(pickle_dumps_path, 'checkpoint.pickle')\n",
    "shared_path = os.path.join('/scratch/juanmoo1/shared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parsed Data\n",
    "\n",
    "Format: \n",
    "{\n",
    "    document_name <str> : {\n",
    "        [\n",
    "            {\n",
    "                \"section\": <str>,\n",
    "                \"subsection\": <str>,\n",
    "                \"header\": <str>,\n",
    "                \"subheader\": <str>,\n",
    "                \"text\": <str>\n",
    "            },\n",
    "            \n",
    "            ...\n",
    "            \n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "}\n",
    "'''\n",
    "data = load_parsed_file(EMA_parsed_path)\n",
    "old_data = load_parsed_file(EMA_old_parsed_path)\n",
    "\n",
    "\n",
    "# Labels\n",
    "'''\n",
    "Dict in form:\n",
    "{\n",
    "    file_name: {\n",
    "        texts: [ <str>, ...],\n",
    "        labels: [ <str>, ...]\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "}\n",
    "'''\n",
    "\n",
    "annotations = utils.parse_spreadsheet(EMA_annotations_path)\n",
    "old_annotations = utils.parse_spreadsheet(EMA_old_annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Data to Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:20<00:00, 64.14s/it]\n",
      "100%|██████████| 68/68 [16:26<00:00, 14.51s/it] \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Iterates through each document in the dataset and compares is to labels with the same file name. Matching is done using fuzzy string matching unless the exact_matching is set to True.\n",
    "'''\n",
    "labels = match_labels(data, annotations, exact_match=False)\n",
    "old_labels = match_labels(old_data, old_annotations, exact_match=False)\n",
    "\n",
    "save_value('data', data, path=checkpoint_path)\n",
    "save_value('old_data', old_data, path=checkpoint_path)\n",
    "save_value('labels', labels, path=checkpoint_path)\n",
    "save_value('old_labels', old_labels, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_value('data', path=checkpoint_path)\n",
    "old_data = load_value('old_data', path=checkpoint_path)\n",
    "labels = load_value('labels', path=checkpoint_path)\n",
    "old_labels = load_value('old_labels', path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean input text\n",
    "data = tokenize_matches(data)\n",
    "old_data = tokenize_matches(old_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Concept Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.unique(data['doc_name'])\n",
    "train_docs = documents[:3]\n",
    "test_docs = documents[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: contraindication ====================\n",
      "Confussion Matrix: \n",
      "[[805   1]\n",
      " [  0   4]]\n",
      "Precision: 0.8\n",
      "Recall: 1.0\n",
      "F1: 0.888888888888889\n",
      "Training Examples Count: 3\n",
      "Test Examples Count: 4\n",
      "\n",
      "==================== Testing Label: populations - adolescent ====================\n",
      "Confussion Matrix: \n",
      "[[730   3]\n",
      " [ 72   5]]\n",
      "Precision: 0.625\n",
      "Recall: 0.06493506493506493\n",
      "F1: 0.11764705882352941\n",
      "Training Examples Count: 12\n",
      "Test Examples Count: 77\n",
      "\n",
      "==================== Testing Label: populations - adult ====================\n",
      "Confussion Matrix: \n",
      "[[727   1]\n",
      " [ 79   3]]\n",
      "Precision: 0.75\n",
      "Recall: 0.036585365853658534\n",
      "F1: 0.0697674418604651\n",
      "Training Examples Count: 6\n",
      "Test Examples Count: 82\n",
      "\n",
      "==================== Testing Label: populations - geriatric ====================\n",
      "Confussion Matrix: \n",
      "[[786   1]\n",
      " [ 18   5]]\n",
      "Precision: 0.8333333333333333\n",
      "Recall: 0.21739130434782608\n",
      "F1: 0.3448275862068965\n",
      "Training Examples Count: 12\n",
      "Test Examples Count: 23\n",
      "\n",
      "==================== Testing Label: populations - paediatric ====================\n",
      "Confussion Matrix: \n",
      "[[734   3]\n",
      " [ 61  12]]\n",
      "Precision: 0.8\n",
      "Recall: 0.1643835616438356\n",
      "F1: 0.2727272727272727\n",
      "Training Examples Count: 23\n",
      "Test Examples Count: 73\n",
      "\n",
      "==================== Testing Label: significant findings - hepatic impairment ====================\n",
      "Confussion Matrix: \n",
      "[[808   0]\n",
      " [  2   0]]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "Training Examples Count: 2\n",
      "Test Examples Count: 2\n",
      "\n",
      "==================== Testing Label: significant findings - pregnancy ====================\n",
      "Confussion Matrix: \n",
      "[[794   0]\n",
      " [  9   7]]\n",
      "Precision: 1.0\n",
      "Recall: 0.43749999999999994\n",
      "F1: 0.608695652173913\n",
      "Training Examples Count: 6\n",
      "Test Examples Count: 16\n",
      "\n",
      "==================== Testing Label: significant findings - renal impairment ====================\n",
      "Confussion Matrix: \n",
      "[[806   0]\n",
      " [  4   0]]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "Training Examples Count: 4\n",
      "Test Examples Count: 4\n",
      "\n",
      "==================== Testing Label: warning ====================\n",
      "Confussion Matrix: \n",
      "[[698  16]\n",
      " [  3  93]]\n",
      "Precision: 0.8532110091743119\n",
      "Recall: 0.96875\n",
      "F1: 0.9073170731707316\n",
      "Training Examples Count: 76\n",
      "Test Examples Count: 96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train = data.loc[data['doc_name'].isin(train_docs)]\n",
    "data_test = data.loc[data['doc_name'].isin(test_docs)]\n",
    "\n",
    "import warnings\n",
    "# Ignore division by zero when calculating F1 score\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning) \n",
    "output_file = os.path.join(shared_path, 'trainNew_testNew.txt')\n",
    "\n",
    "with open(output_file, 'w') as outFile:\n",
    "    \n",
    "    for l in labels:\n",
    "        if l != 'other':\n",
    "            summary = '=' * 20 + ' Testing Label: ' + str(l) + ' ' + '=' * 20 + '\\n'\n",
    "            out = ''\n",
    "            \n",
    "\n",
    "            train_count = data_train[l].sum()\n",
    "            test_count = data_test[l].sum()\n",
    "                        \n",
    "\n",
    "            if train_count > 1:\n",
    "                \n",
    "                params = svm_train(data_train, l)\n",
    "                output = svm_test(data_test, params, verbose=True)\n",
    "\n",
    "                precision = output['precision']\n",
    "                recall = output['recall']\n",
    "                cm = output['cm']             \n",
    "\n",
    "                all_predicted = output['all_predicted']\n",
    "                actual_positive = output['actual_positive']\n",
    "                true_positive = output['true_positive']\n",
    "                false_positive = output['false_positive']\n",
    "                false_negative = output['false_negative']\n",
    "\n",
    "\n",
    "                summary += 'Confussion Matrix: \\n'\n",
    "                summary += str(cm) + '\\n'\n",
    "                \n",
    "                \n",
    "                \n",
    "                summary += 'Precision: ' + str(precision) + '\\n'\n",
    "                summary += 'Recall: ' + str(recall) + '\\n'\n",
    "                summary += 'F1: ' + str(2 * (precision * recall)/(precision + recall)) + '\\n'\n",
    "\n",
    "\n",
    "                summary += 'Training Examples Count: ' + str(train_count) + '\\n'\n",
    "                summary += 'Test Examples Count: ' + str(test_count) + '\\n'\n",
    "                \n",
    "                example_head = '-' * 20 + ' %s ' + '-' * 20 + '\\n'\n",
    "                example_format = '# %d. DOC: %s\\nSECTION: %s \\nSUBSECTION: %s\\n HEADER: %s\\nSUBHEADER: %s \\nTEXT: %s \\n\\n\\n'\n",
    "                \n",
    "                \n",
    "                out += example_head%('PREDICTED')\n",
    "                for index, (doc, sec, subsec, head, subhead, text) in all_predicted.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, head, subhead, text)\n",
    "                out += '\\n'\n",
    "                \n",
    "                out += example_head%('TRUE POSITIVE')\n",
    "                for index, (doc, sec, subsec, head, subhead, text) in true_positive.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, head, subhead, text)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE NEGATIVE')\n",
    "                for index, (doc, sec, subsec, head, subhead, text) in false_negative.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, head, subhead, text)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE POSITIVE')\n",
    "                for index, (doc, sec, subsec, head, subhead, text) in false_positive.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, head, subhead, text)\n",
    "                out += '\\n'\n",
    "                \n",
    "            else:\n",
    "                summary += 'There were only ' + str(train_count) + ' training examples. 2 or more are needed to train the model.'\n",
    "                summary += '\\n'\n",
    "            \n",
    "            print(summary)\n",
    "            outFile.write(summary + '\\n')\n",
    "            outFile.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Old / Test New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: contraindication ====================\n",
      "Confussion Matrix: \n",
      "[[804   2]\n",
      " [  0   4]]\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 1.0\n",
      "F1: 0.8\n",
      "Training Examples Count: 122\n",
      "Test Examples Count: 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: populations - adolescent ====================\n",
      "Confussion Matrix: \n",
      "[[722  11]\n",
      " [  1  76]]\n",
      "Precision: 0.8735632183908046\n",
      "Recall: 0.987012987012987\n",
      "F1: 0.9268292682926829\n",
      "Training Examples Count: 99\n",
      "Test Examples Count: 77\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: populations - adult ====================\n",
      "Confussion Matrix: \n",
      "[[718  10]\n",
      " [  8  74]]\n",
      "Precision: 0.8809523809523808\n",
      "Recall: 0.902439024390244\n",
      "F1: 0.8915662650602411\n",
      "Training Examples Count: 82\n",
      "Test Examples Count: 82\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: populations - geriatric ====================\n",
      "Confussion Matrix: \n",
      "[[787   0]\n",
      " [ 14   9]]\n",
      "Precision: 1.0\n",
      "Recall: 0.391304347826087\n",
      "F1: 0.5625\n",
      "Training Examples Count: 27\n",
      "Test Examples Count: 23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: populations - paediatric ====================\n",
      "Confussion Matrix: \n",
      "[[724  13]\n",
      " [  2  71]]\n",
      "Precision: 0.8452380952380952\n",
      "Recall: 0.9726027397260275\n",
      "F1: 0.9044585987261148\n",
      "Training Examples Count: 109\n",
      "Test Examples Count: 73\n",
      "\n",
      "==================== Testing Label: significant findings - hepatic impairment ====================\n",
      "Confussion Matrix: \n",
      "[[803   5]\n",
      " [  0   2]]\n",
      "Precision: 0.2857142857142857\n",
      "Recall: 1.0\n",
      "F1: 0.4444444444444445\n",
      "Training Examples Count: 27\n",
      "Test Examples Count: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: significant findings - pregnancy ====================\n",
      "Confussion Matrix: \n",
      "[[794   0]\n",
      " [  5  11]]\n",
      "Precision: 1.0\n",
      "Recall: 0.6875\n",
      "F1: 0.8148148148148148\n",
      "Training Examples Count: 252\n",
      "Test Examples Count: 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: significant findings - renal impairment ====================\n",
      "Confussion Matrix: \n",
      "[[806   0]\n",
      " [  2   2]]\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1: 0.6666666666666666\n",
      "Training Examples Count: 28\n",
      "Test Examples Count: 4\n",
      "\n",
      "==================== Testing Label: warning ====================\n",
      "Confussion Matrix: \n",
      "[[709   5]\n",
      " [ 50  46]]\n",
      "Precision: 0.9019607843137255\n",
      "Recall: 0.47916666666666663\n",
      "F1: 0.6258503401360543\n",
      "Training Examples Count: 561\n",
      "Test Examples Count: 96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train = old_data\n",
    "data_test = data.loc[data['doc_name'].isin(test_docs)]\n",
    "\n",
    "import warnings\n",
    "# Ignore division by zero when calculating F1 score\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning) \n",
    "output_file = os.path.join(shared_path, 'trainOld_testNew.txt')\n",
    "\n",
    "with open(output_file, 'w') as outFile:\n",
    "    \n",
    "    for l in labels:\n",
    "        if l != 'other':\n",
    "            summary = '=' * 20 + ' Testing Label: ' + str(l) + ' ' + '=' * 20 + '\\n'\n",
    "            out = ''\n",
    "            \n",
    "\n",
    "            train_count = data_train[l].sum()\n",
    "            test_count = data_test[l].sum()\n",
    "                        \n",
    "\n",
    "            if train_count > 1:\n",
    "                \n",
    "                params = svm_train(data_train, l)\n",
    "                output = svm_test(data_test, params, verbose=True)\n",
    "\n",
    "                precision = output['precision']\n",
    "                recall = output['recall']\n",
    "                cm = output['cm']             \n",
    "\n",
    "                all_predicted = output['all_predicted']\n",
    "                actual_positive = output['actual_positive']\n",
    "                true_positive = output['true_positive']\n",
    "                false_positive = output['false_positive']\n",
    "                false_negative = output['false_negative']\n",
    "\n",
    "\n",
    "                summary += 'Confussion Matrix: \\n'\n",
    "                summary += str(cm) + '\\n'\n",
    "                \n",
    "                \n",
    "                \n",
    "                summary += 'Precision: ' + str(precision) + '\\n'\n",
    "                summary += 'Recall: ' + str(recall) + '\\n'\n",
    "                summary += 'F1: ' + str(2 * (precision * recall)/(precision + recall)) + '\\n'\n",
    "\n",
    "\n",
    "                summary += 'Training Examples Count: ' + str(train_count) + '\\n'\n",
    "                summary += 'Test Examples Count: ' + str(test_count) + '\\n'\n",
    "                \n",
    "                example_head = '-' * 20 + ' %s ' + '-' * 20 + '\\n'\n",
    "                example_format = '# %d. DOC: %s\\nSECTION: %s \\nSUBSECTION: %s\\n HEADER: %s\\nSUBHEADER: %s \\nTEXT: %s \\n\\n\\n'\n",
    "                \n",
    "                \n",
    "                out += example_head%('PREDICTED')\n",
    "                for index, (doc, sec, subsec, head, subhead, text) in all_predicted.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, head, subhead, text)\n",
    "                out += '\\n'\n",
    "                \n",
    "                out += example_head%('TRUE POSITIVE')\n",
    "                for index, (doc, sec, subsec, head, subhead, text) in true_positive.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, head, subhead, text)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE NEGATIVE')\n",
    "                for index, (doc, sec, subsec, head, subhead, text) in false_negative.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, head, subhead, text)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE POSITIVE')\n",
    "                for index, (doc, sec, subsec, head, subhead, text) in false_positive.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, head, subhead, text)\n",
    "                out += '\\n'\n",
    "                \n",
    "            else:\n",
    "                summary += 'There were only ' + str(train_count) + ' training examples. 2 or more are needed to train the model.'\n",
    "                summary += '\\n'\n",
    "            \n",
    "            print(summary)\n",
    "            outFile.write(summary + '\\n')\n",
    "            outFile.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
