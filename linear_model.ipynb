{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "import utils, json\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from linear_model import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '/scratch/juanmoo1/bayer'\n",
    "\n",
    "EMA_annotations_path = os.path.join(data_path, 'VendorEMAforMIT/newLabels/annotations.xlsx')\n",
    "EMA_old_annotations_path = os.path.join(data_path, 'VendorEMAforMIT/annotations.xlsx')\n",
    "\n",
    "EMA_old_parsed_path = os.path.join(data_path, './VendorEMAforMIT/Labels/parsed.json')\n",
    "EMA_parsed_path = os.path.join(data_path, './VendorEMAforMIT/newLabels/parsed.json')\n",
    "\n",
    "pickle_dumps_path = os.path.join(data_path, 'pickle_dumps/')\n",
    "checkpoint_path = os.path.join(pickle_dumps_path, 'checkpoint.pickle')\n",
    "shared_path = os.path.join('/scratch/juanmoo1/shared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parsed Data\n",
    "\n",
    "Format: \n",
    "{\n",
    "    document_name <str> : {\n",
    "        [\n",
    "            {\n",
    "                \"section\": <str>,\n",
    "                \"subsection\": <str>,\n",
    "                \"header\": <str>,\n",
    "                \"subheader\": <str>,\n",
    "                \"text\": <str>\n",
    "            },\n",
    "            \n",
    "            ...\n",
    "            \n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "}\n",
    "'''\n",
    "data = load_parsed_file(EMA_parsed_path)\n",
    "old_data = load_parsed_file(EMA_old_parsed_path)\n",
    "\n",
    "\n",
    "# Labels\n",
    "'''\n",
    "Dict in form:\n",
    "{\n",
    "    file_name: {\n",
    "        texts: [ <str>, ...],\n",
    "        labels: [ <str>, ...]\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "}\n",
    "'''\n",
    "\n",
    "annotations = utils.parse_spreadsheet(EMA_annotations_path)\n",
    "old_annotations = utils.parse_spreadsheet(EMA_old_annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Data to Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.91it/s]\n",
      "100%|██████████| 68/68 [00:13<00:00,  4.92it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Iterates through each document in the dataset and compares is to labels with the same file name. Matching is done using fuzzy string matching unless the exact_matching is set to True.\n",
    "'''\n",
    "labels = match_labels(data, annotations, exact_match=True)\n",
    "old_labels = match_labels(old_data, old_annotations, exact_match=True)\n",
    "\n",
    "save_value('data', data, path=checkpoint_path)\n",
    "save_value('old_data', old_data, path=checkpoint_path)\n",
    "save_value('labels', labels, path=checkpoint_path)\n",
    "save_value('old_labels', old_labels, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_value('data', path=checkpoint_path)\n",
    "old_data = load_value('old_data', path=checkpoint_path)\n",
    "labels = load_value('labels', path=checkpoint_path)\n",
    "old_labels = load_value('old_labels', path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean input text\n",
    "data = tokenize_matches(data)\n",
    "old_data = tokenize_matches(old_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Concept Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.unique(data['doc_name'])\n",
    "train_docs = documents[:3]\n",
    "test_docs = documents[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: contraindication ====================\n",
      "Confussion Matrix: \n",
      "[[978   5]\n",
      " [  4   5]]\n",
      "Precision: 0.5555555555555556\n",
      "Recall: 0.5\n",
      "F1: 0.5263157894736842\n",
      "Training Examples Count: 4\n",
      "Test Examples Count: 9\n",
      "\n",
      "==================== Testing Label: populations - adolescent ====================\n",
      "Confussion Matrix: \n",
      "[[899   5]\n",
      " [ 75  13]]\n",
      "Precision: 0.14772727272727273\n",
      "Recall: 0.7222222222222223\n",
      "F1: 0.24528301886792456\n",
      "Training Examples Count: 17\n",
      "Test Examples Count: 88\n",
      "\n",
      "==================== Testing Label: populations - adult ====================\n",
      "Confussion Matrix: \n",
      "[[888   3]\n",
      " [ 92   9]]\n",
      "Precision: 0.0891089108910891\n",
      "Recall: 0.75\n",
      "F1: 0.15929203539823011\n",
      "Training Examples Count: 8\n",
      "Test Examples Count: 101\n",
      "\n",
      "==================== Testing Label: populations - geriatric ====================\n",
      "Confussion Matrix: \n",
      "[[954   7]\n",
      " [ 26   5]]\n",
      "Precision: 0.16129032258064516\n",
      "Recall: 0.4166666666666667\n",
      "F1: 0.2325581395348837\n",
      "Training Examples Count: 13\n",
      "Test Examples Count: 31\n",
      "\n",
      "==================== Testing Label: populations - paediatric ====================\n",
      "Confussion Matrix: \n",
      "[[903   5]\n",
      " [ 63  21]]\n",
      "Precision: 0.25\n",
      "Recall: 0.8076923076923077\n",
      "F1: 0.38181818181818183\n",
      "Training Examples Count: 28\n",
      "Test Examples Count: 84\n",
      "\n",
      "==================== Testing Label: significant findings - hepatic impairment ====================\n",
      "Confussion Matrix: \n",
      "[[989   0]\n",
      " [  3   0]]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "Training Examples Count: 2\n",
      "Test Examples Count: 3\n",
      "\n",
      "==================== Testing Label: significant findings - pregnancy ====================\n",
      "Confussion Matrix: \n",
      "[[956   4]\n",
      " [  8  24]]\n",
      "Precision: 0.75\n",
      "Recall: 0.8571428571428571\n",
      "F1: 0.7999999999999999\n",
      "Training Examples Count: 11\n",
      "Test Examples Count: 32\n",
      "\n",
      "==================== Testing Label: significant findings - renal impairment ====================\n",
      "Confussion Matrix: \n",
      "[[987   0]\n",
      " [  5   0]]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "Training Examples Count: 4\n",
      "Test Examples Count: 5\n",
      "\n",
      "==================== Testing Label: warning ====================\n",
      "Confussion Matrix: \n",
      "[[806  18]\n",
      " [  4 164]]\n",
      "Precision: 0.9761904761904762\n",
      "Recall: 0.9010989010989011\n",
      "F1: 0.937142857142857\n",
      "Training Examples Count: 99\n",
      "Test Examples Count: 168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train = data.loc[data['doc_name'].isin(train_docs)]\n",
    "data_test = data.loc[data['doc_name'].isin(test_docs)]\n",
    "\n",
    "import warnings\n",
    "# Ignore division by zero when calculating F1 score\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning) \n",
    "output_file = os.path.join(shared_path, 'trainNew_testNew.txt')\n",
    "\n",
    "with open(output_file, 'w') as outFile:\n",
    "    \n",
    "    for l in labels:\n",
    "        if l != 'other':\n",
    "            summary = '=' * 20 + ' Testing Label: ' + str(l) + ' ' + '=' * 20 + '\\n'\n",
    "            out = ''\n",
    "            \n",
    "\n",
    "            train_count = data_train[l].sum()\n",
    "            test_count = data_test[l].sum()\n",
    "                        \n",
    "\n",
    "            if train_count > 1:\n",
    "                \n",
    "                params = svm_train(data_train, l)\n",
    "                output = svm_test(data_test, params, verbose=True)\n",
    "\n",
    "                precision = output['precision']\n",
    "                recall = output['recall']\n",
    "                cm = output['cm']             \n",
    "\n",
    "                all_predicted = output['all_predicted']\n",
    "                actual_positive = output['actual_positive']\n",
    "                true_positive = output['true_positive']\n",
    "                false_positive = output['false_positive']\n",
    "                false_negative = output['false_negative']\n",
    "\n",
    "\n",
    "                summary += 'Confussion Matrix: \\n'\n",
    "                summary += str(cm) + '\\n'\n",
    "                \n",
    "                \n",
    "                \n",
    "                summary += 'Precision: ' + str(precision) + '\\n'\n",
    "                summary += 'Recall: ' + str(recall) + '\\n'\n",
    "                summary += 'F1: ' + str(2 * (precision * recall)/(precision + recall)) + '\\n'\n",
    "\n",
    "\n",
    "                summary += 'Training Examples Count: ' + str(train_count) + '\\n'\n",
    "                summary += 'Test Examples Count: ' + str(test_count) + '\\n'\n",
    "                \n",
    "                example_head = '-' * 20 + ' %s ' + '-' * 20 + '\\n'\n",
    "                example_format = '# %d. DOC: %s\\nSECTION: %s \\nSUBSECTION: %s\\nSUBHEADER: %s \\nTEXT: %s \\n\\n\\n'\n",
    "                \n",
    "                \n",
    "                out += example_head%('PREDICTED')\n",
    "                for index, (doc, sec, subsec, subhead, text) in all_predicted.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, subhead, text)\n",
    "                out += '\\n'\n",
    "                \n",
    "                out += example_head%('TRUE POSITIVE')\n",
    "                for index, (doc, sec, subsec, subhead, text) in true_positive.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, subhead, text)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE NEGATIVE')\n",
    "                for index, (doc, sec, subsec, subhead, text) in false_negative.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, subhead, text)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE POSITIVE')\n",
    "                for index, (doc, sec, subsec, subhead, text) in false_positive.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, subhead, text)\n",
    "                out += '\\n'\n",
    "                \n",
    "            else:\n",
    "                summary += 'There were only ' + str(train_count) + ' training examples. 2 or more are needed to train the model.'\n",
    "                summary += '\\n'\n",
    "            \n",
    "            print(summary)\n",
    "            outFile.write(summary + '\\n')\n",
    "            outFile.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Old / Test New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: contraindication ====================\n",
      "Confussion Matrix: \n",
      "[[972  11]\n",
      " [  4   5]]\n",
      "Precision: 0.5555555555555556\n",
      "Recall: 0.3125\n",
      "F1: 0.39999999999999997\n",
      "Training Examples Count: 186\n",
      "Test Examples Count: 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: populations - adolescent ====================\n",
      "Confussion Matrix: \n",
      "[[892  12]\n",
      " [  2  86]]\n",
      "Precision: 0.9772727272727273\n",
      "Recall: 0.8775510204081631\n",
      "F1: 0.9247311827956989\n",
      "Training Examples Count: 116\n",
      "Test Examples Count: 88\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: populations - adult ====================\n",
      "Confussion Matrix: \n",
      "[[879  12]\n",
      " [ 10  91]]\n",
      "Precision: 0.9009900990099009\n",
      "Recall: 0.883495145631068\n",
      "F1: 0.892156862745098\n",
      "Training Examples Count: 100\n",
      "Test Examples Count: 101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: populations - geriatric ====================\n",
      "Confussion Matrix: \n",
      "[[961   0]\n",
      " [ 18  13]]\n",
      "Precision: 0.41935483870967744\n",
      "Recall: 1.0\n",
      "F1: 0.5909090909090909\n",
      "Training Examples Count: 37\n",
      "Test Examples Count: 31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: populations - paediatric ====================\n",
      "Confussion Matrix: \n",
      "[[895  13]\n",
      " [  3  81]]\n",
      "Precision: 0.9642857142857143\n",
      "Recall: 0.8617021276595745\n",
      "F1: 0.9101123595505619\n",
      "Training Examples Count: 144\n",
      "Test Examples Count: 84\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: significant findings - hepatic impairment ====================\n",
      "Confussion Matrix: \n",
      "[[984   5]\n",
      " [  1   2]]\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.2857142857142857\n",
      "F1: 0.4\n",
      "Training Examples Count: 34\n",
      "Test Examples Count: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: significant findings - pregnancy ====================\n",
      "Confussion Matrix: \n",
      "[[960   0]\n",
      " [  6  26]]\n",
      "Precision: 0.8125\n",
      "Recall: 1.0\n",
      "F1: 0.896551724137931\n",
      "Training Examples Count: 368\n",
      "Test Examples Count: 32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/juanmoo1/envs/bayer/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: significant findings - renal impairment ====================\n",
      "Confussion Matrix: \n",
      "[[987   0]\n",
      " [  3   2]]\n",
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "F1: 0.5714285714285715\n",
      "Training Examples Count: 35\n",
      "Test Examples Count: 5\n",
      "\n",
      "==================== Testing Label: warning ====================\n",
      "Confussion Matrix: \n",
      "[[812  12]\n",
      " [ 80  88]]\n",
      "Precision: 0.5238095238095238\n",
      "Recall: 0.88\n",
      "F1: 0.6567164179104478\n",
      "Training Examples Count: 796\n",
      "Test Examples Count: 168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train = old_data\n",
    "data_test = data.loc[data['doc_name'].isin(test_docs)]\n",
    "\n",
    "import warnings\n",
    "# Ignore division by zero when calculating F1 score\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning) \n",
    "output_file = os.path.join(shared_path, 'trainOld_testNew.txt')\n",
    "\n",
    "with open(output_file, 'w') as outFile:\n",
    "    \n",
    "    for l in labels:\n",
    "        if l != 'other':\n",
    "            summary = '=' * 20 + ' Testing Label: ' + str(l) + ' ' + '=' * 20 + '\\n'\n",
    "            out = ''\n",
    "            \n",
    "\n",
    "            train_count = data_train[l].sum()\n",
    "            test_count = data_test[l].sum()\n",
    "                        \n",
    "\n",
    "            if train_count > 1:\n",
    "                \n",
    "                params = svm_train(data_train, l)\n",
    "                output = svm_test(data_test, params, verbose=True)\n",
    "\n",
    "                precision = output['precision']\n",
    "                recall = output['recall']\n",
    "                cm = output['cm']             \n",
    "\n",
    "                all_predicted = output['all_predicted']\n",
    "                actual_positive = output['actual_positive']\n",
    "                true_positive = output['true_positive']\n",
    "                false_positive = output['false_positive']\n",
    "                false_negative = output['false_negative']\n",
    "\n",
    "\n",
    "                summary += 'Confussion Matrix: \\n'\n",
    "                summary += str(cm) + '\\n'\n",
    "                \n",
    "                \n",
    "                \n",
    "                summary += 'Precision: ' + str(precision) + '\\n'\n",
    "                summary += 'Recall: ' + str(recall) + '\\n'\n",
    "                summary += 'F1: ' + str(2 * (precision * recall)/(precision + recall)) + '\\n'\n",
    "\n",
    "\n",
    "                summary += 'Training Examples Count: ' + str(train_count) + '\\n'\n",
    "                summary += 'Test Examples Count: ' + str(test_count) + '\\n'\n",
    "                \n",
    "                example_head = '-' * 20 + ' %s ' + '-' * 20 + '\\n'\n",
    "                example_format = '# %d. DOC: %s\\nSECTION: %s \\nSUBSECTION: %s\\nSUBHEADER: %s \\nTEXT: %s \\n\\n\\n'\n",
    "                \n",
    "                \n",
    "                out += example_head%('PREDICTED')\n",
    "                for index, (doc, sec, subsec, subhead, text) in all_predicted.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, subhead, text)\n",
    "                out += '\\n'\n",
    "                \n",
    "                out += example_head%('TRUE POSITIVE')\n",
    "                for index, (doc, sec, subsec, subhead, text) in true_positive.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, subhead, text)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE NEGATIVE')\n",
    "                for index, (doc, sec, subsec, subhead, text) in false_negative.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, subhead, text)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE POSITIVE')\n",
    "                for index, (doc, sec, subsec, subhead, text) in false_positive.iterrows():\n",
    "                    out += example_format%(index, doc, sec, subsec, subhead, text)\n",
    "                out += '\\n'\n",
    "                \n",
    "            else:\n",
    "                summary += 'There were only ' + str(train_count) + ' training examples. 2 or more are needed to train the model.'\n",
    "                summary += '\\n'\n",
    "            \n",
    "            print(summary)\n",
    "            outFile.write(summary + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
