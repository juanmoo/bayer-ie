{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "import utils, json\n",
    "from tqdm import tqdm\n",
    "from utils import save_value, load_value, load_env_keys, match_labels, tokenize_string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '/scratch/juanmoo1'\n",
    "EMA_dump_path = os.path.join(data_path, './jsons/new_EMA_dump.json')\n",
    "EMA_old_dump_path = os.path.join(data_path, './jsons/EMA_dump.json')\n",
    "\n",
    "EMA_annotations_path = os.path.join(data_path, './bayer/VendorEMAforMIT/new_annotations/annotations.xlsx')\n",
    "EMA_old_annotations_path = os.path.join(data_path, './bayer/VendorEMAforMIT/annotations.xlsx')\n",
    "\n",
    "pickle_dumps_path = os.path.join(data_path, './pickle_dumps/')\n",
    "checkpoint_path = os.path.join(pickle_dumps_path, 'checkpoint.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data\n",
    "'''\n",
    "Format:\n",
    "{\n",
    "    document_name <str>: {\n",
    "                            element_text: <str> (raw text),\n",
    "                            element_tag: <str> (TEI XML tag)\n",
    "                          },\n",
    "                          \n",
    "    ...\n",
    "}\n",
    "'''\n",
    "data = json.loads(open(EMA_dump_path, 'r').read())\n",
    "old_data = json.loads(open(EMA_old_dump_path, 'r').read())\n",
    "\n",
    "\n",
    "# Labels\n",
    "'''\n",
    "Dict in form:\n",
    "{\n",
    "    file_name: {\n",
    "        texts: [ <str>, ...],\n",
    "        labels: [ <str>, ...]\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "}\n",
    "'''\n",
    "annotations = utils.parse_spreadsheet(EMA_annotations_path)\n",
    "old_annotations = utils.parse_spreadsheet(EMA_old_annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Data to Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:57<00:00, 107.40s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Iterates through each document in the dataset and compares is to labels with the same file name. Matching is done using fuzzy string matching unless the exact_matching is set to True.\n",
    "'''\n",
    "\n",
    "labeled_raw_documents = match_labels(data, annotations, exact_match=False)\n",
    "save_value('labeled_raw_documents', labeled_raw_documents, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_documents = load_value('labeled_raw_documents', path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean input text\n",
    "processed_documents = dict()\n",
    "processed_document_list = []\n",
    "\n",
    "\n",
    "for doc_name in labeled_raw_documents:\n",
    "    texts = [tokenize_string(raw) for raw in labeled_raw_documents[doc_name]['texts']]\n",
    "    labels = labeled_raw_documents[doc_name]['labels']\n",
    "    head1 = [tokenize_string(raw) for raw in labeled_raw_documents[doc_name]['head1']]\n",
    "    head2 = [tokenize_string(raw) for raw in labeled_raw_documents[doc_name]['head2']]\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        processed_document_list.append([doc_name, head1[i], head2[i], labels[i], texts[i]])\n",
    "\n",
    "\n",
    "data = pd.DataFrame(processed_document_list, columns=['document', 'head1', 'head2', 'label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['other', 'Populations - Geriatric', 'Populations - Paediatric',\n",
       "       'Populations - Adolescent', 'Contraindication', 'Warning',\n",
       "       'Significant Findings - Pregnancy',\n",
       "       'Significant Findings - Hepatic Impairment', 'Populations - Adult',\n",
       "       'Significant Findings - Renal Impairment'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline of feature engineering and model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = Pipeline([('vectorizer', CountVectorizer()),\n",
    " ('tfidf', TfidfTransformer()),\n",
    " ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced')))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Search with Header augmented feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack, vstack\n",
    "from functools import reduce\n",
    "\n",
    "# Search Params\n",
    "ngram_configs = [(1, 1), (2, 2), (1, 2), (1, 3), (1, 4), (1, 5)]\n",
    "tfidf_configs = [True, False]\n",
    "vectorizer_stopwords_configs = ['english', None]\n",
    "min_df_configs = [0] + [10**(-n) for n in range(3, 4)]\n",
    "\n",
    "total_config_count = len(ngram_configs) * len(tfidf_configs) * len(vectorizer_stopwords_configs) * len(min_df_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def multi_svm_train(doc_list, config=None):\n",
    "    train_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    train_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "    train_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "    train_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    if config is None:\n",
    "        config = load_value('best_config_header', path=checkpoint_path)\n",
    "        \n",
    "    text_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    header_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.fit_transform(train_texts)\n",
    "    tokenized_header1 = header_tokenizer.fit_transform(train_header1)\n",
    "    tokenized_header2 = header_tokenizer.transform(train_header2)\n",
    "    X_train = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    Y_train = train_labels\n",
    "\n",
    "    model = Pipeline([('tfidf', TfidfTransformer(use_idf=config['tfidf_config'])), ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return (model, text_tokenizer, header_tokenizer)\n",
    "\n",
    "def multi_svm_test(model, text_tokenizer, header_tokenizer, doc_list):\n",
    "    test_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    test_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "    test_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "    test_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.transform(test_texts)\n",
    "    tokenized_header1 = header_tokenizer.transform(test_header1)\n",
    "    tokenized_header2 = header_tokenizer.transform(test_header2)\n",
    "    X_test = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    Y_test = test_labels\n",
    "    pred = model.predict(X_test)\n",
    "    cm = np.array(confusion_matrix(Y_test, pred))\n",
    "    \n",
    "    # Diagonal elemetns were correctly classified\n",
    "    diagonal = cm.diagonal()\n",
    "    \n",
    "    # Input class Counts\n",
    "    class_sum = cm.sum(axis=1)\n",
    "    \n",
    "    # Predicted class counts\n",
    "    pred_sum = cm.sum(axis=0)\n",
    "    \n",
    "    # Per-class performance w/ no-examples -> 0 perf\n",
    "    precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
    "    recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n",
    "    \n",
    "    # Frequency Weighted Performance\n",
    "    c_freq = cm.sum(axis=1)/cm.sum()\n",
    "    pres = c_freq * precision\n",
    "    rec = c_freq * recall\n",
    "    \n",
    "    # Remove 'other' Category\n",
    "    c_freq = c_freq[0:1] + c_freq[2:]\n",
    "    pres = pres[0:1] + pres[2:]\n",
    "    rec = rec[0:1] + rec[2:]\n",
    "    \n",
    "    return pres.sum()/c_freq.sum(), rec.sum()/c_freq.sum()\n",
    "\n",
    "def cross_validation(doc_list, train_algo, test_algo, k, verbose=False, config=None):\n",
    "    N = len(doc_list)\n",
    "    size = N//k\n",
    "    indeces = list(range(N))\n",
    "    random.shuffle(indeces)\n",
    "    all_indeces = set(indeces)\n",
    "    \n",
    "    pres_list = []\n",
    "    rec_list = []\n",
    "    \n",
    "    for j in range(N//size):\n",
    "        train_indeces = indeces[j * size:(j + 1) * size] + indeces[size * k + j: size * k + j + 1]\n",
    "        test_indeces = list(all_indeces - set(train_indeces))\n",
    "        \n",
    "        train_docs = [doc_list[i] for i in train_indeces]\n",
    "        test_docs = [doc_list[i] for i in test_indeces]\n",
    "        \n",
    "        if verbose:\n",
    "            print('Fold %d starting!'%(j + 1))\n",
    "        \n",
    "        m, tt, ht = train_algo(train_docs, config=config)\n",
    "        pres, rec = test_algo(m, tt, ht, test_docs)\n",
    "        \n",
    "        pres_list.append(pres)\n",
    "        rec_list.append(rec)\n",
    "        \n",
    "        if verbose:\n",
    "            print('precision:', pres)\n",
    "            print('recall:', rec)\n",
    "            print('-' * 10 + '\\n')\n",
    "    \n",
    "    return sum(pres_list)/k, sum(rec_list)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "best_config = None\n",
    "best_f1 = -1\n",
    "\n",
    "count = 0\n",
    "for stop_config in vectorizer_stopwords_configs:\n",
    "    for ngram_config in ngram_configs:\n",
    "        for min_df_config in min_df_configs:\n",
    "            for tfidf_config in tfidf_configs:\n",
    "                count += 1\n",
    "                config = {\n",
    "                            'stop_config': stop_config,\n",
    "                            'ngram_config': ngram_config,\n",
    "                            'tfidf_config': tfidf_config,\n",
    "                            'min_df_config': min_df_config\n",
    "                         }\n",
    "                print('Progress: ' + str(count) + '/' + str(total_config_count), '\\t =>', count/total_config_count)\n",
    "                print('Testing configuration:', config)\n",
    "                pres, rec = cross_validation(list(processed_documents), multi_svm_train, multi_svm_test, 10, verbose=False)\n",
    "                f1 = 2 * (pres * rec)/(pres + rec)\n",
    "                \n",
    "                print('Precision: %f \\t Recall: %f, F1: %f'%(pres, rec, f1))\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_config = config\n",
    "                    best_f1 = f1\n",
    "\n",
    "\n",
    "save_value('best_config_header', config, path=checkpoint_path)\n",
    "save_value('best_score_header', f1, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_value('best_config_header', path=checkpoint_path)\n",
    "acc = load_value('best_score_header', path=checkpoint_path)\n",
    "print(config)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_value('best_config_header', path=checkpoint_path)\n",
    "\n",
    "avg_precision, avg_recall = cross_validation(list(processed_documents), multi_svm_train, multi_svm_test, 10, verbose=True, config=config)\n",
    "print('Average Precision:', avg_precision)\n",
    "print('Average Recall:', avg_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_value('best_header_config', path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Concept Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack, vstack\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def svm_train(train_docs, data, label, config=None):\n",
    "    train_data = data.loc[data['document'].isin(train_docs)]\n",
    "    \n",
    "    if config is None:\n",
    "        config = {\n",
    "            'ngram_config': (1, 4),\n",
    "            'stop_config': 'english',\n",
    "            'tfidf_config': True\n",
    "        }\n",
    "        \n",
    "    text_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    head1_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    head2_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.fit_transform(train_data['text'])\n",
    "    tokenized_head1 = head1_tokenizer.fit_transform(train_data['head1'])\n",
    "    tokenized_head2 = head2_tokenizer.fit_transform(train_data['head2'])\n",
    "    \n",
    "    X_train = hstack([tokenized_texts, tokenized_head1, tokenized_head2])\n",
    "    Y_train = (train_data['label'] == label)\n",
    "    \n",
    "    model = Pipeline([('tfidf', TfidfTransformer(use_idf=config['tfidf_config'])), ('clf', LinearSVC(class_weight=\"balanced\"))])\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'tt': text_tokenizer,\n",
    "        'h1t': head1_tokenizer,\n",
    "        'h2t': head2_tokenizer,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "def svm_test(test_docs, data, params):\n",
    "    \n",
    "    test_data = data.loc[data['document'].isin(test_docs)]\n",
    "    \n",
    "    tt = params['tt'].transform(test_data['text'])\n",
    "    th1 = params['h1t'].transform(test_data['head1'])\n",
    "    th2 = params['h2t'].transform(test_data['head2'])\n",
    "    \n",
    "    X_test = hstack([tt, th1, th2])\n",
    "    Y_test = (test_data['label'] == params['label'])\n",
    "   \n",
    "    pred = params['model'].predict(X_test)\n",
    "    cm = np.array(confusion_matrix(Y_test, pred))\n",
    "    \n",
    "    # Diagonal elemetns were correctly classified\n",
    "    diagonal = cm.diagonal()\n",
    "    \n",
    "    # Input class Counts\n",
    "    class_sum = cm.sum(axis=1)\n",
    "    \n",
    "    # Predicted class counts\n",
    "    pred_sum = cm.sum(axis=0)\n",
    "    \n",
    "    # Per-class performance w/ no-examples -> 0 perf\n",
    "    precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
    "    recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n",
    "    \n",
    "    # Frequency Weighted Performance\n",
    "    c_freq = cm.sum(axis=1)/cm.sum()\n",
    "    pres = c_freq * precision\n",
    "    rec = c_freq * recall\n",
    "    \n",
    "    # Remove 'other' Category\n",
    "    c_freq = c_freq[1:]\n",
    "    pres = pres[1:] \n",
    "    rec = rec[1:]\n",
    "    \n",
    "    return pres.sum()/c_freq.sum(), rec.sum()/c_freq.sum(), cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(pd.unique(data['label']))\n",
    "documents = pd.unique(data['document'])\n",
    "train_docs = documents[:3]\n",
    "test_docs = documents[3:]\n",
    "\n",
    "data_train = data.loc[data['document'].isin(train_docs)]\n",
    "data_test = data.loc[data['document'].isin(test_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abasaglar-previously-abasria-epar-product-information_en',\n",
       "       'aclasta-epar-product-information_en'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Testing Label: Contraindication ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171   0]\n",
      " [  1   2]]\n",
      "\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 1.0\n",
      "F1: 0.8\n",
      "\n",
      "Training Examples Count: 6\n",
      "Test Examples Count: 3\n",
      "\n",
      "========== Testing Label: Populations - Adolescent ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[163   6]\n",
      " [  4   1]]\n",
      "\n",
      "Precision: 0.2\n",
      "Recall: 0.14285714285714285\n",
      "F1: 0.16666666666666666\n",
      "\n",
      "Training Examples Count: 31\n",
      "Test Examples Count: 5\n",
      "\n",
      "========== Testing Label: Populations - Adult ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[172   0]\n",
      " [  2   0]]\n",
      "\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "\n",
      "Training Examples Count: 43\n",
      "Test Examples Count: 2\n",
      "\n",
      "========== Testing Label: Populations - Geriatric ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171   0]\n",
      " [  3   0]]\n",
      "\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "\n",
      "Training Examples Count: 18\n",
      "Test Examples Count: 3\n",
      "\n",
      "========== Testing Label: Populations - Paediatric ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[161   4]\n",
      " [  7   2]]\n",
      "\n",
      "Precision: 0.2222222222222222\n",
      "Recall: 0.3333333333333333\n",
      "F1: 0.26666666666666666\n",
      "\n",
      "Training Examples Count: 33\n",
      "Test Examples Count: 9\n",
      "\n",
      "========== Testing Label: Significant Findings - Hepatic Impairment ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[174]]\n",
      "\n",
      "Precision: nan\n",
      "Recall: nan\n",
      "F1: nan\n",
      "\n",
      "Training Examples Count: 3\n",
      "Test Examples Count: 0\n",
      "\n",
      "========== Testing Label: Significant Findings - Pregnancy ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[167   4]\n",
      " [  0   3]]\n",
      "\n",
      "Precision: 1.0\n",
      "Recall: 0.42857142857142855\n",
      "F1: 0.6\n",
      "\n",
      "Training Examples Count: 14\n",
      "Test Examples Count: 3\n",
      "\n",
      "========== Testing Label: Significant Findings - Renal Impairment ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171   0]\n",
      " [  3   0]]\n",
      "\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "\n",
      "Training Examples Count: 2\n",
      "Test Examples Count: 3\n",
      "\n",
      "========== Testing Label: Warning ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[146   3]\n",
      " [  5  20]]\n",
      "\n",
      "Precision: 0.8\n",
      "Recall: 0.8695652173913043\n",
      "F1: 0.8333333333333333\n",
      "\n",
      "Training Examples Count: 80\n",
      "Test Examples Count: 25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Ignore division by zero when calculating F1 score\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning) \n",
    "\n",
    "for l in labels:\n",
    "    if l != 'other':\n",
    "        print('=' * 10 , 'Testing Label:', l, '=' * 10)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        params = svm_train(train_docs, data, l)\n",
    "        precision, recall, cm = svm_test(test_docs, data, params)\n",
    "        \n",
    "        train_count = (data_train['label'] == l).sum()\n",
    "        test_count = (data_test['label'] == l).sum()\n",
    "        \n",
    "        print('Confusion Matrix:')\n",
    "        print(cm)\n",
    "        print()\n",
    "        \n",
    "        print('Precision:', precision)\n",
    "        print('Recall:', recall)\n",
    "        print('F1:', 2 * (precision * recall)/(precision + recall))\n",
    "        print()\n",
    "        \n",
    "        print('Training Examples Count:', train_count)\n",
    "        print('Test Examples Count:', test_count)\n",
    "        print()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
