{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "import utils, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '/scratch/juanmoo1'\n",
    "EMA_dump_path = os.path.join(data_path, './jsons/EMA_dump.json')\n",
    "EMA_xmls_path = os.path.join(data_path, './xmls/')\n",
    "EMA_annotations_path = os.path.join(data_path, './bayer/VendorEMAforMIT/annotations.xlsx')\n",
    "\n",
    "pickle_dumps_path = os.path.join(data_path, './pickle_dumps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data\n",
    "'''\n",
    "Format:\n",
    "{\n",
    "    document_name <str>: {\n",
    "                            element_text: <str> (raw text),\n",
    "                            element_tag: <str> (TEI XML tag)\n",
    "                          },\n",
    "                          \n",
    "    ...\n",
    "}\n",
    "'''\n",
    "data = json.loads(open(EMA_dump_path, 'r').read())\n",
    "\n",
    "\n",
    "# Labels\n",
    "'''\n",
    "Dict in form:\n",
    "{\n",
    "    file_name: {\n",
    "        texts: [ <str>, ...],\n",
    "        labels: [ <str>, ...]\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "}\n",
    "'''\n",
    "annotations = utils.parse_spreadsheet(EMA_annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "def save_value(key, val):\n",
    "    with open(os.path.join(pickle_dumps_path, 'checkpoint.pickle'), 'rb') as f:\n",
    "        try:\n",
    "            saved_env = pickle.load(f)\n",
    "            saved_env.keys()\n",
    "        except:\n",
    "            saved_env = dict()\n",
    "            \n",
    "    saved_env[key] = val\n",
    "    \n",
    "    with open(os.path.join(pickle_dumps_path, 'checkpoint.pickle'), 'wb') as f:\n",
    "        f.write(pickle.dumps(saved_env))\n",
    "    \n",
    "def load_value(key):\n",
    "    with open(os.path.join(pickle_dumps_path, 'checkpoint.pickle'), 'rb') as f:\n",
    "        s = f.read()\n",
    "        try:\n",
    "            saved_env = pickle.loads(s)\n",
    "            ans = saved_env[key]\n",
    "        except:\n",
    "            ans = None\n",
    "        \n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Data to Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each document in 'data', iterate through the paragraphs and check to see if there's \n",
    "an elemenet the paragraphs of the element in 'labels' with the same name.\n",
    "'''\n",
    "from matplotlib.pyplot import hist, title\n",
    "from fuzzywuzzy import fuzz\n",
    "from time import time as time\n",
    "\n",
    "def f1(piece, whole):\n",
    "    return piece in whole\n",
    "\n",
    "def f2(piece, whole):\n",
    "    return piece.lower() in whole.lower()\n",
    "\n",
    "def f3(piece, whole):\n",
    "    threshold = 95\n",
    "    return fuzz.partial_ratio(piece, whole) >= threshold\n",
    "\n",
    "def contains_test(piece, whole):\n",
    "    # test if 'piece' is a member of 'whole'\n",
    "    \n",
    "    test_functions = [f1, f2, f3]\n",
    "    \n",
    "    i = 0\n",
    "    return test_functions[i](piece, whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_frac = []\n",
    "elapsed_times = []\n",
    "\n",
    "labeled_raw_documents = {}\n",
    "\n",
    "for parsed_doc_name in data:\n",
    "    start_time = time()\n",
    "\n",
    "    parsed_doc = data[parsed_doc_name]\n",
    "    label_doc = annotations[parsed_doc_name] #label doc w/ same name\n",
    "    \n",
    "    matchings = [] # el: [parsed paragraph, label_paragraph_id, label]\n",
    "    other = []\n",
    "    \n",
    "    paragraphs = []\n",
    "    labels = []\n",
    "    tags = []\n",
    "    \n",
    "    for parsed_p, tag in zip(parsed_doc['element_text'], parsed_doc['element_tag']):\n",
    "        found = False\n",
    "        for i, label_p in enumerate(label_doc['texts']):\n",
    "            if contains_test(parsed_p, label_p): #Match\n",
    "                found = True\n",
    "                matchings.append([parsed_p, i, label_doc['labels'][i]])\n",
    "                \n",
    "                paragraphs.append(parsed_p)\n",
    "                labels.append(label_doc['labels'][i])\n",
    "                tags.append(tag)\n",
    "                break\n",
    "        if not found:\n",
    "            other.append(parsed_p)\n",
    "            \n",
    "            paragraphs.append(parsed_p)\n",
    "            labels.append('other')\n",
    "            tags.append(tag)\n",
    "            \n",
    "    \n",
    "    tot_time = time() - start_time\n",
    "    elapsed_times.append(tot_time)\n",
    "            \n",
    "    num_texts = len(parsed_doc['element_text'])        \n",
    "    match_frac.append(len(matchings)/num_texts)\n",
    "    \n",
    "    labeled_raw_documents[parsed_doc_name] = {\n",
    "        'matches': matchings,\n",
    "        'other': other,\n",
    "        'paragraphs': paragraphs,\n",
    "        'labels': labels,\n",
    "        'tags': tags\n",
    "    }        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(pickle_dumps_path, 'EMA.pickle'), 'wb') as f:\n",
    "    f.write(pickle.dumps(labeled_raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(pickle_dumps_path, 'EMA_fuzzy.pickle'), 'rb') as f:\n",
    "    labeled_raw_documents = pickle.loads(f.read())\n",
    "\n",
    "save_value('labeled_raw_documents', labeled_raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(elapsed_times)\n",
    "print(sum(elapsed_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Matches:\", sum(match_frac)/len(match_frac))\n",
    "hist(match_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjecture:\n",
    "The fraction of text in the labels is much smaller than all text. Thus, we should fail to find labels for most of extracted paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ratios = []\n",
    "for parsed_doc_name in data:\n",
    "    parsed_doc = data[parsed_doc_name]\n",
    "    label_doc = annotations[parsed_doc_name]\n",
    "    \n",
    "    parsed_text = ''.join(e.strip() for e in parsed_doc['element_text'])\n",
    "    label_text = ''.join(e.strip() for e in label_doc['texts'])\n",
    "    \n",
    "    ratio = len(label_text)/len(parsed_text)\n",
    "    data_ratios.append(ratio)\n",
    "data_ratios.sort()\n",
    "\n",
    "# Cut lowest 5% and top 5%\n",
    "start = int(0.05 * len(data_ratios))\n",
    "end = int(0.95 * len(data_ratios))\n",
    "\n",
    "title(\"Total Text to Labeled Text Ratio\")\n",
    "hist(data_ratios[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "There seem to be missing documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anames = list(annotations.keys())\n",
    "fnames = list(data.keys())\n",
    "\n",
    "# from scipy.spatial.distance import hamming\n",
    "\n",
    "# There's extra files referenced in the annotation spreadsheet\n",
    "print('anotation names count:', len(anames))\n",
    "print('document count:', len(fnames))\n",
    "\n",
    "missing_ann = set(fnames) - set(anames)\n",
    "missing_docs = set(anames) - set(fnames)\n",
    "\n",
    "print('missing annotations count:', len(missing_ann))\n",
    "print('missing doc count:', len(missing_docs))\n",
    "print('missing docs:')\n",
    "for doc_name in missing_docs:\n",
    "    print('-', doc_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\n\", \"\", string)    \n",
    "    string = re.sub(r\"\\r\", \"\", string) \n",
    "    string = re.sub(r\"[0-9]\", \"digit\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean input text\n",
    "processed_documents = {}\n",
    "\n",
    "for doc_name in labeled_raw_documents:\n",
    "    texts = [clean_str(raw) for raw in labeled_raw_documents[doc_name]['paragraphs']]\n",
    "    labels = [l.lower() for l in labeled_raw_documents[doc_name]['labels']]\n",
    "    tags = [t.lower() for t in labeled_raw_documents[doc_name]['tags']]\n",
    "    \n",
    "    processed_documents[doc_name] = {\n",
    "        'texts': texts,\n",
    "        'labels': labels,\n",
    "        'tags': tags\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match text with the previous header\n",
    "for doc_name in processed_documents:\n",
    "    texts = processed_documents[doc_name]['texts']\n",
    "    tags = processed_documents[doc_name]['tags']\n",
    "    \n",
    "    # First Header\n",
    "    header_index = [-1] * len(texts)    \n",
    "    last_header = 0\n",
    "    \n",
    "    while(last_header < len(tags) and tags[last_header] != 'head'):\n",
    "        last_header += 1\n",
    "        \n",
    "    i = last_header + 1\n",
    "    \n",
    "    while i < len(tags):\n",
    "        header_index[i] = last_header\n",
    "        if tags[i] == 'head':\n",
    "            last_header = i\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Second Header\n",
    "    header2 = [-1] * len(header_index)\n",
    "    last_header = 0\n",
    "    while(last_header < len(tags) and header_index[last_header] == -1):\n",
    "        last_header += 1\n",
    "    \n",
    "    i = last_header + 1\n",
    "    while i < len(header2):\n",
    "        header2[i] = header_index[last_header]\n",
    "        if header_index[i] != -1:\n",
    "            last_header = i\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "    processed_documents[doc_name]['header_index'] = header_index\n",
    "    processed_documents[doc_name]['header_index_2'] = header2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from functools import reduce\n",
    "names = list(processed_documents.keys())\n",
    "\n",
    "rs = ShuffleSplit(n_splits=1, test_size = 0.3)\n",
    "split = next(rs.split(names))\n",
    "\n",
    "train_docs = list(map(lambda i: names[i], split[0]))\n",
    "test_docs = list(map(lambda i: names[i], split[1]))\n",
    "\n",
    "\n",
    "X_train = reduce(lambda l, dname: l + processed_documents[dname]['texts'], [[]] + train_docs)\n",
    "Y_train = reduce(lambda l, name: l + processed_documents[name]['labels'], [[]] + train_docs)\n",
    "\n",
    "\n",
    "X_test = reduce(lambda l, dname: l + processed_documents[dname]['texts'], [[]] + test_docs)\n",
    "Y_test = reduce(lambda l, name: l + processed_documents[name]['labels'], [[]] + test_docs)\n",
    "\n",
    "X = X_train + X_test\n",
    "Y = Y_train + Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text + Header\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Count Tokenizer\n",
    "all_texts = set()\n",
    "all_headers = set()\n",
    "for doc_name in processed_documents:\n",
    "    texts = processed_documents[doc_name]['texts']\n",
    "    labels = processed_documents[doc_name]['labels']\n",
    "    header_index = processed_documents[doc_name]['header_index']\n",
    "    \n",
    "    all_texts = all_texts | set(texts)\n",
    "    all_headers = all_headers | set([texts[i] for i in header_index if i != -1])\n",
    "\n",
    "all_texts = sorted(list(all_texts))\n",
    "all_headers = sorted(list(all_headers))\n",
    "\n",
    "text_index = {el:i for i, el in enumerate(all_texts)}\n",
    "header_index = {el:i for i, el in enumerate(all_headers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
    "all_texts = text_vectorizer.fit_transform(all_texts)\n",
    "\n",
    "header_vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
    "all_headers = header_vectorizer.fit_transform(all_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove data classes with insufficient examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_count = {el: Y.count(el) for el in set(Y)}\n",
    "\n",
    "min_count = 10\n",
    "X_train, Y_train = zip(*[(X_train[i], Y_train[i]) for i in range(len(X_train)) if Y_count[Y_train[i]] >= min_count])\n",
    "X_test, Y_test = zip(*[(X_test[i], Y_test[i]) for i in range(len(X_test)) if Y_count[Y_test[i]] >= min_count])\n",
    "\n",
    "X = X_train + X_test\n",
    "Y = Y_train + Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(Y))\n",
    "labels.sort(key=lambda x: Y.count(x))\n",
    "\n",
    "for l in labels:\n",
    "    print('-', l[:20], '| frequency:', Y.count(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline of feature engineering and model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = Pipeline([('vectorizer', CountVectorizer()),\n",
    " ('tfidf', TfidfTransformer()),\n",
    " ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced')))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramater selection\n",
    "# Params 1 #\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "               'tfidf__use_idf': [False, True],\n",
    "               'vectorizer__min_df': [0, 0.0001, 0.00001],\n",
    "               'vectorizer__stop_words':[None, 'english']\n",
    "             }\n",
    "gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X, Y)\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)\n",
    "\n",
    "# Save best found configuration\n",
    "save_value('best_params1', gs_clf_svm.best_params_)\n",
    "save_value('best_score1', gs_clf_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = load_value('best_params1')\n",
    "best_score = load_value('best_score1')\n",
    "\n",
    "#Training of Final Model\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,4), min_df = 0, stop_words=None)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "#Test\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(Y_test, pred)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(Y_test, pred)\n",
    "\n",
    "class_count = len(cm)\n",
    "\n",
    "class_accuracy = [cm[i][i]/sum(cm[i]) if sum(cm[i]) > 0 else 0 for i in range(class_count)]\n",
    "w_acc = [class_accuracy[i] * sum(cm[i]) for i in range(len(cm))]\n",
    "acc_no_other = (sum(w_acc) - w_acc[1])/(sum(sum(cm[i]) for i in range(class_count)) - sum(cm[1]))\n",
    "                                        \n",
    "print(cm)\n",
    "print('Class Acuracy:', class_accuracy)\n",
    "print('Overall Accuracy:', accuracy)\n",
    "print('Accuracy excluding other:', acc_no_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dict()\n",
    "for real, prediction in zip(Y_test, pred):\n",
    "    if real not in categories:\n",
    "        categories[real] = [0,0] #total, correct\n",
    "    categories[real][0] += 1\n",
    "    categories[real][1] += (real == prediction)\n",
    "    \n",
    "for c in categories:\n",
    "    tot = categories[c][0]\n",
    "    corr = categories[c][1]\n",
    "    acc = corr/tot\n",
    "#     print(c + ' =>', 'total:', tot, '\\t\\t correct:', corr, '\\t\\t accuracy:', acc)\n",
    "    print(c + ': acc', acc, 'total:', tot)\n",
    "#     print(c + ': correct:', corr, \"\\t || accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO's 04/01/2020\n",
    "* Remove common words from text\n",
    "* Remove classes with few examples\n",
    "* Append Corresponding headers to examples\n",
    "* Limit length of feature vector // Filter low freq words\n",
    "* Stop words\n",
    "* Append BOG of corresponding header\n",
    "* Explore other models ?\n",
    "    ** Try decision tree\n",
    "    ** try small ff nn\n",
    "    \n",
    "* Separate train / test based on documents\n",
    "* Consider paragraphs w/o labels and 'other' label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Search with Header augmented feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack, vstack\n",
    "from functools import reduce\n",
    "\n",
    "# Search Params\n",
    "ngram_configs = [(1, 1), (2, 2), (1, 2), (1, 3), (1, 4), (1, 5)]\n",
    "tfidf_configs = [True, False]\n",
    "vectorizer_stopwords_configs = ['english', None]\n",
    "min_df_configs = [0] + [10**(-n) for n in range(3, 4)]\n",
    "\n",
    "total_config_count = len(ngram_configs) * len(tfidf_configs) * len(vectorizer_stopwords_configs) * len(min_df_configs)\n",
    "\n",
    "# Vocab & Data Lists\n",
    "texts = set()\n",
    "headers = set()\n",
    "\n",
    "text_list = []\n",
    "label_list = []\n",
    "header1_list = []\n",
    "header2_list = []\n",
    "\n",
    "for doc_name in processed_documents:\n",
    "    doc = processed_documents[doc_name]\n",
    "    texts = texts | set(doc['texts'])\n",
    "    headers = headers | set(doc['texts'][i] for i in doc['header_index'] if i != -1)\n",
    "    \n",
    "    header1 = [doc['texts'][i] if i != -1 else \"\" for i in doc['header_index']]\n",
    "    header2 = [doc['texts'][i] if i != -1 else \"\" for i in doc['header_index_2']]\n",
    "    \n",
    "    doc['header1'] = header1\n",
    "    doc['header2'] = header2\n",
    "    \n",
    "    text_list.extend(doc['texts'])\n",
    "    label_list.extend(doc['labels'])\n",
    "    header1_list.extend(header1)\n",
    "    header2_list.extend(header2)\n",
    "\n",
    "save_value('processed_documents', processed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def multi_svm_train(doc_list, config=None):\n",
    "    train_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    train_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "    train_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "    train_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    if config is None:\n",
    "        config = load_value('best_config_header')\n",
    "        \n",
    "    text_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    header_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.fit_transform(train_texts)\n",
    "    tokenized_header1 = header_tokenizer.fit_transform(train_header1)\n",
    "    tokenized_header2 = header_tokenizer.transform(train_header2)\n",
    "    X_train = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    Y_train = train_labels\n",
    "\n",
    "    model = Pipeline([('tfidf', TfidfTransformer(use_idf=config['tfidf_config'])), ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return (model, text_tokenizer, header_tokenizer)\n",
    "\n",
    "def multi_svm_test(model, text_tokenizer, header_tokenizer, doc_list):\n",
    "    test_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    test_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "    test_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "    test_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.transform(test_texts)\n",
    "    tokenized_header1 = header_tokenizer.transform(test_header1)\n",
    "    tokenized_header2 = header_tokenizer.transform(test_header2)\n",
    "    X_test = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    Y_test = test_labels\n",
    "    pred = model.predict(X_test)\n",
    "    cm = np.array(confusion_matrix(Y_test, pred))\n",
    "    \n",
    "    # Diagonal elemetns were correctly classified\n",
    "    diagonal = cm.diagonal()\n",
    "    \n",
    "    # Input class Counts\n",
    "    class_sum = cm.sum(axis=1)\n",
    "    \n",
    "    # Predicted class counts\n",
    "    pred_sum = cm.sum(axis=0)\n",
    "    \n",
    "    # Per-class performance w/ no-examples -> 0 perf\n",
    "    precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
    "    recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n",
    "    \n",
    "    # Frequency Weighted Performance\n",
    "    c_freq = cm.sum(axis=1)/cm.sum()\n",
    "    pres = c_freq * precision\n",
    "    rec = c_freq * recall\n",
    "    \n",
    "    # Remove 'other' Category\n",
    "    c_freq = c_freq[0:1] + c_freq[2:]\n",
    "    pres = pres[0:1] + pres[2:]\n",
    "    rec = rec[0:1] + rec[2:]\n",
    "    \n",
    "    return pres.sum()/c_freq.sum(), rec.sum()/c_freq.sum()\n",
    "\n",
    "def cross_validation(doc_list, train_algo, test_algo, k, verbose=False, config=None):\n",
    "    N = len(doc_list)\n",
    "    size = N//k\n",
    "    indeces = list(range(N))\n",
    "    random.shuffle(indeces)\n",
    "    all_indeces = set(indeces)\n",
    "    \n",
    "    pres_list = []\n",
    "    rec_list = []\n",
    "    \n",
    "    for j in range(N//size):\n",
    "        train_indeces = indeces[j * size:(j + 1) * size] + indeces[size * k + j: size * k + j + 1]\n",
    "        test_indeces = list(all_indeces - set(train_indeces))\n",
    "        \n",
    "        train_docs = [doc_list[i] for i in train_indeces]\n",
    "        test_docs = [doc_list[i] for i in test_indeces]\n",
    "        \n",
    "        if verbose:\n",
    "            print('Fold %d starting!'%(j + 1))\n",
    "        \n",
    "        m, tt, ht = train_algo(train_docs, config=config)\n",
    "        pres, rec = test_algo(m, tt, ht, test_docs)\n",
    "        \n",
    "        pres_list.append(pres)\n",
    "        rec_list.append(rec)\n",
    "        \n",
    "        if verbose:\n",
    "            print('precision:', pres)\n",
    "            print('recall:', rec)\n",
    "            print('-' * 10 + '\\n')\n",
    "    \n",
    "    return sum(pres_list)/k, sum(rec_list)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1/48 \t => 0.020833333333333332\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 1), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.508181 \t Recall: 0.330853, F1: 0.400778\n",
      "Progress: 2/48 \t => 0.041666666666666664\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 1), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.459985 \t Recall: 0.351600, F1: 0.398555\n",
      "Progress: 3/48 \t => 0.0625\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 1), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.513561 \t Recall: 0.338875, F1: 0.408319\n",
      "Progress: 4/48 \t => 0.08333333333333333\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 1), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.494716 \t Recall: 0.339161, F1: 0.402430\n",
      "Progress: 5/48 \t => 0.10416666666666667\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (2, 2), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.492521 \t Recall: 0.319235, F1: 0.387382\n",
      "Progress: 6/48 \t => 0.125\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (2, 2), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.482743 \t Recall: 0.339623, F1: 0.398729\n",
      "Progress: 7/48 \t => 0.14583333333333334\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (2, 2), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.486087 \t Recall: 0.357290, F1: 0.411854\n",
      "Progress: 8/48 \t => 0.16666666666666666\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (2, 2), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.468703 \t Recall: 0.348960, F1: 0.400063\n",
      "Progress: 9/48 \t => 0.1875\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 2), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.502426 \t Recall: 0.337892, F1: 0.404051\n",
      "Progress: 10/48 \t => 0.20833333333333334\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 2), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.504910 \t Recall: 0.353296, F1: 0.415711\n",
      "Progress: 11/48 \t => 0.22916666666666666\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 2), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.508045 \t Recall: 0.332027, F1: 0.401596\n",
      "Progress: 12/48 \t => 0.25\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 2), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.468990 \t Recall: 0.357092, F1: 0.405462\n",
      "Progress: 13/48 \t => 0.2708333333333333\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 3), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.470167 \t Recall: 0.343188, F1: 0.396766\n",
      "Progress: 14/48 \t => 0.2916666666666667\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 3), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.482061 \t Recall: 0.365319, F1: 0.415648\n",
      "Progress: 15/48 \t => 0.3125\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 3), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.478505 \t Recall: 0.369786, F1: 0.417179\n",
      "Progress: 16/48 \t => 0.3333333333333333\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 3), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.484749 \t Recall: 0.353228, F1: 0.408667\n",
      "Progress: 17/48 \t => 0.3541666666666667\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 4), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.493177 \t Recall: 0.362131, F1: 0.417615\n",
      "Progress: 18/48 \t => 0.375\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 4), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.494262 \t Recall: 0.341858, F1: 0.404170\n",
      "Progress: 19/48 \t => 0.3958333333333333\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 4), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.465713 \t Recall: 0.336850, F1: 0.390936\n",
      "Progress: 20/48 \t => 0.4166666666666667\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 4), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.516188 \t Recall: 0.337804, F1: 0.408366\n",
      "Progress: 21/48 \t => 0.4375\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 5), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.486264 \t Recall: 0.337608, F1: 0.398525\n",
      "Progress: 22/48 \t => 0.4583333333333333\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 5), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.507015 \t Recall: 0.349115, F1: 0.413504\n",
      "Progress: 23/48 \t => 0.4791666666666667\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 5), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.515944 \t Recall: 0.328103, F1: 0.401122\n",
      "Progress: 24/48 \t => 0.5\n",
      "Testing configuration: {'stop_config': 'english', 'ngram_config': (1, 5), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.500341 \t Recall: 0.339820, F1: 0.404746\n",
      "Progress: 25/48 \t => 0.5208333333333334\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 1), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.485061 \t Recall: 0.334929, F1: 0.396251\n",
      "Progress: 26/48 \t => 0.5416666666666666\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 1), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.506479 \t Recall: 0.355275, F1: 0.417612\n",
      "Progress: 27/48 \t => 0.5625\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 1), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.510545 \t Recall: 0.314028, F1: 0.388869\n",
      "Progress: 28/48 \t => 0.5833333333333334\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 1), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.500065 \t Recall: 0.338538, F1: 0.403745\n",
      "Progress: 29/48 \t => 0.6041666666666666\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (2, 2), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.479269 \t Recall: 0.348884, F1: 0.403812\n",
      "Progress: 30/48 \t => 0.625\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (2, 2), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.507005 \t Recall: 0.328590, F1: 0.398750\n",
      "Progress: 31/48 \t => 0.6458333333333334\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (2, 2), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.486507 \t Recall: 0.336067, F1: 0.397530\n",
      "Progress: 32/48 \t => 0.6666666666666666\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (2, 2), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.498578 \t Recall: 0.353414, F1: 0.413629\n",
      "Progress: 33/48 \t => 0.6875\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 2), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.469621 \t Recall: 0.353235, F1: 0.403197\n",
      "Progress: 34/48 \t => 0.7083333333333334\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 2), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.484031 \t Recall: 0.337762, F1: 0.397879\n",
      "Progress: 35/48 \t => 0.7291666666666666\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 2), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.486213 \t Recall: 0.325526, F1: 0.389966\n",
      "Progress: 36/48 \t => 0.75\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 2), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.488779 \t Recall: 0.337476, F1: 0.399274\n",
      "Progress: 37/48 \t => 0.7708333333333334\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 3), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.512651 \t Recall: 0.341680, F1: 0.410058\n",
      "Progress: 38/48 \t => 0.7916666666666666\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 3), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.489756 \t Recall: 0.339140, F1: 0.400764\n",
      "Progress: 39/48 \t => 0.8125\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 3), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.482570 \t Recall: 0.363627, F1: 0.414739\n",
      "Progress: 40/48 \t => 0.8333333333333334\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 3), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.505584 \t Recall: 0.345994, F1: 0.410835\n",
      "Progress: 41/48 \t => 0.8541666666666666\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 4), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.474136 \t Recall: 0.342116, F1: 0.397450\n",
      "Progress: 42/48 \t => 0.875\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 4), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.499551 \t Recall: 0.330468, F1: 0.397788\n",
      "Progress: 43/48 \t => 0.8958333333333334\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 4), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.472519 \t Recall: 0.343259, F1: 0.397648\n",
      "Progress: 44/48 \t => 0.9166666666666666\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 4), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.480583 \t Recall: 0.331781, F1: 0.392554\n",
      "Progress: 45/48 \t => 0.9375\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 5), 'tfidf_config': True, 'min_df_config': 0}\n",
      "Precision: 0.494918 \t Recall: 0.346048, F1: 0.407307\n",
      "Progress: 46/48 \t => 0.9583333333333334\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 5), 'tfidf_config': False, 'min_df_config': 0}\n",
      "Precision: 0.513078 \t Recall: 0.335292, F1: 0.405556\n",
      "Progress: 47/48 \t => 0.9791666666666666\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 5), 'tfidf_config': True, 'min_df_config': 0.001}\n",
      "Precision: 0.485319 \t Recall: 0.359393, F1: 0.412969\n",
      "Progress: 48/48 \t => 1.0\n",
      "Testing configuration: {'stop_config': None, 'ngram_config': (1, 5), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "Precision: 0.480528 \t Recall: 0.326387, F1: 0.388735\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "best_config = None\n",
    "best_f1 = -1\n",
    "\n",
    "count = 0\n",
    "for stop_config in vectorizer_stopwords_configs:\n",
    "    for ngram_config in ngram_configs:\n",
    "        for min_df_config in min_df_configs:\n",
    "            for tfidf_config in tfidf_configs:\n",
    "                count += 1\n",
    "                config = {\n",
    "                            'stop_config': stop_config,\n",
    "                            'ngram_config': ngram_config,\n",
    "                            'tfidf_config': tfidf_config,\n",
    "                            'min_df_config': min_df_config\n",
    "                         }\n",
    "                print('Progress: ' + str(count) + '/' + str(total_config_count), '\\t =>', count/total_config_count)\n",
    "                print('Testing configuration:', config)\n",
    "                pres, rec = cross_validation(list(processed_documents), multi_svm_train, multi_svm_test, 10, verbose=False)\n",
    "                f1 = 2 * (pres * rec)/(pres + rec)\n",
    "                \n",
    "                print('Precision: %f \\t Recall: %f, F1: %f'%(pres, rec, f1))\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_config = config\n",
    "                    best_f1 = f1\n",
    "\n",
    "\n",
    "save_value('best_config_header', config)\n",
    "save_value('best_score_header', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stop_config': None, 'ngram_config': (1, 5), 'tfidf_config': False, 'min_df_config': 0.001}\n",
      "0.38873530664544026\n"
     ]
    }
   ],
   "source": [
    "config = load_value('best_config_header')\n",
    "acc = load_value('best_score_header')\n",
    "print(config)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 starting!\n",
      "precision: 0.5682751540041068\n",
      "recall: 0.3829214393926515\n",
      "----------\n",
      "\n",
      "Fold 2 starting!\n",
      "precision: 0.5383583690987125\n",
      "recall: 0.3825319397816723\n",
      "----------\n",
      "\n",
      "Fold 3 starting!\n",
      "precision: 0.5305598356445815\n",
      "recall: 0.3483386192620248\n",
      "----------\n",
      "\n",
      "Fold 4 starting!\n",
      "precision: 0.36693333333333333\n",
      "recall: 0.3828944078814136\n",
      "----------\n",
      "\n",
      "Fold 5 starting!\n",
      "precision: 0.41299677765843174\n",
      "recall: 0.39406887913286665\n",
      "----------\n",
      "\n",
      "Fold 6 starting!\n",
      "precision: 0.24435721295387636\n",
      "recall: 0.25730713940904704\n",
      "----------\n",
      "\n",
      "Fold 7 starting!\n",
      "precision: 0.3486954401365521\n",
      "recall: 0.3690686865156796\n",
      "----------\n",
      "\n",
      "Fold 8 starting!\n",
      "precision: 0.5224511357633386\n",
      "recall: 0.37791317843509137\n",
      "----------\n",
      "\n",
      "Fold 9 starting!\n",
      "precision: 0.5613605442176871\n",
      "recall: 0.41313832954909074\n",
      "----------\n",
      "\n",
      "Fold 10 starting!\n",
      "precision: 0.5951770138532579\n",
      "recall: 0.3407857569142289\n",
      "----------\n",
      "\n",
      "Average Precision: 0.4689164816663878\n",
      "Average Recall: 0.3648968376273766\n"
     ]
    }
   ],
   "source": [
    "config = load_value('best_config_header')\n",
    "\n",
    "avg_precision, avg_recall = cross_validation(list(processed_documents), multi_svm_train, multi_svm_test, 10, verbose=True, config=config)\n",
    "print('Average Precision:', avg_precision)\n",
    "print('Average Recall:', avg_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_name = 'elmiron-epar-product-information_en'\n",
    "doc = processed_documents[doc_name]\n",
    "\n",
    "texts = doc['texts']\n",
    "\n",
    "headers = doc['header1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_config': 'english',\n",
       " 'ngram_config': (1, 3),\n",
       " 'tfidf_config': False,\n",
       " 'min_df_config': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_value('best_header_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_value('best_header_config', {'stop_config': 'english', 'ngram_config': (1, 3), 'tfidf_config': False, 'min_df_config': 0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
