{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "import utils, json\n",
    "from tqdm import tqdm\n",
    "from utils import save_value, load_value, load_env_keys, match_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '/scratch/juanmoo1'\n",
    "EMA_dump_path = os.path.join(data_path, './jsons/EMA_dump.json')\n",
    "EMA_xmls_path = os.path.join(data_path, './xmls/')\n",
    "EMA_annotations_path = os.path.join(data_path, './bayer/VendorEMAforMIT/annotations.xlsx')\n",
    "pickle_dumps_path = os.path.join(data_path, './pickle_dumps/')\n",
    "checkpoint_path = os.path.join(pickle_dumps_path, 'checkpoint.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data\n",
    "'''\n",
    "Format:\n",
    "{\n",
    "    document_name <str>: {\n",
    "                            element_text: <str> (raw text),\n",
    "                            element_tag: <str> (TEI XML tag)\n",
    "                          },\n",
    "                          \n",
    "    ...\n",
    "}\n",
    "'''\n",
    "data = json.loads(open(EMA_dump_path, 'r').read())\n",
    "\n",
    "\n",
    "# Labels\n",
    "'''\n",
    "Dict in form:\n",
    "{\n",
    "    file_name: {\n",
    "        texts: [ <str>, ...],\n",
    "        labels: [ <str>, ...]\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "}\n",
    "'''\n",
    "annotations = utils.parse_spreadsheet(EMA_annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Data to Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Iterates through each document in the dataset and compares is to labels with the same file name. Matching is done using fuzzy string matching unless the exact_matching is set to True.\n",
    "'''\n",
    "\n",
    "labeled_raw_documents = match_labels(data, annotations)\n",
    "save_value('labeled_raw_documents', labeled_raw_documents, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_documents = load_value('labeled_raw_documents', path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjecture:\n",
    "The fraction of text in the labels is much smaller than all text. Thus, we should fail to find labels for most of extracted paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import title, hist\n",
    "data_ratios = []\n",
    "for parsed_doc_name in data:\n",
    "    parsed_doc = data[parsed_doc_name]\n",
    "    label_doc = annotations[parsed_doc_name]\n",
    "    \n",
    "    parsed_text = ''.join(e.strip() for e in parsed_doc['element_text'])\n",
    "    label_text = ''.join(e.strip() for e in label_doc['texts'])\n",
    "    \n",
    "    ratio = len(label_text)/len(parsed_text)\n",
    "    data_ratios.append(ratio)\n",
    "data_ratios.sort()\n",
    "\n",
    "# Cut lowest 5% and top 5%\n",
    "start = int(0.05 * len(data_ratios))\n",
    "end = int(0.95 * len(data_ratios))\n",
    "\n",
    "title(\"Total Text to Labeled Text Ratio\")\n",
    "hist(data_ratios[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### There seem to be missing documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anames = list(annotations.keys())\n",
    "fnames = list(data.keys())\n",
    "\n",
    "# from scipy.spatial.distance import hamming\n",
    "\n",
    "# There's extra files referenced in the annotation spreadsheet\n",
    "print('anotation names count:', len(anames))\n",
    "print('document count:', len(fnames))\n",
    "\n",
    "missing_ann = set(fnames) - set(anames)\n",
    "missing_docs = set(anames) - set(fnames)\n",
    "\n",
    "print('missing annotations count:', len(missing_ann))\n",
    "print('missing doc count:', len(missing_docs))\n",
    "print('missing docs:')\n",
    "for doc_name in missing_docs:\n",
    "    print('-', doc_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\n\", \"\", string)    \n",
    "    string = re.sub(r\"\\r\", \"\", string) \n",
    "    string = re.sub(r\"[0-9]\", \"digit\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, spacy\n",
    "\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "MAX_CHARS = 20000\n",
    "\n",
    "def clean_str(comment):\n",
    "    comment = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(comment))\n",
    "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
    "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
    "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
    "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
    "    if (len(comment) > MAX_CHARS):\n",
    "        comment = comment[:MAX_CHARS]\n",
    "    return ' '.join([x.text.lower() for x in NLP.tokenizer(comment) if x.text != \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean input text\n",
    "processed_documents = {}\n",
    "\n",
    "for doc_name in labeled_raw_documents:\n",
    "    texts = [clean_str(raw) for raw in labeled_raw_documents[doc_name]['paragraphs']]\n",
    "    labels = [l.lower() for l in labeled_raw_documents[doc_name]['labels']]\n",
    "    tags = [t.lower() for t in labeled_raw_documents[doc_name]['tags']]\n",
    "    \n",
    "    processed_documents[doc_name] = {\n",
    "        'texts': texts,\n",
    "        'labels': labels,\n",
    "        'tags': tags\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match text with the previous header\n",
    "for doc_name in processed_documents:\n",
    "    texts = processed_documents[doc_name]['texts']\n",
    "    tags = processed_documents[doc_name]['tags']\n",
    "    \n",
    "    # First Header\n",
    "    header_index = [-1] * len(texts)    \n",
    "    last_header = 0\n",
    "    \n",
    "    while(last_header < len(tags) and tags[last_header] != 'head'):\n",
    "        last_header += 1\n",
    "        \n",
    "    i = last_header + 1\n",
    "    \n",
    "    while i < len(tags):\n",
    "        header_index[i] = last_header\n",
    "        if tags[i] == 'head':\n",
    "            last_header = i\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Second Header\n",
    "    header2 = [-1] * len(header_index)\n",
    "    last_header = 0\n",
    "    while(last_header < len(tags) and header_index[last_header] == -1):\n",
    "        last_header += 1\n",
    "    \n",
    "    i = last_header + 1\n",
    "    while i < len(header2):\n",
    "        header2[i] = header_index[last_header]\n",
    "        if header_index[i] != -1:\n",
    "            last_header = i\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "    processed_documents[doc_name]['header_index'] = header_index\n",
    "    processed_documents[doc_name]['header_index_2'] = header2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from functools import reduce\n",
    "names = list(processed_documents.keys())\n",
    "\n",
    "rs = ShuffleSplit(n_splits=1, test_size = 0.3)\n",
    "split = next(rs.split(names))\n",
    "\n",
    "train_docs = list(map(lambda i: names[i], split[0]))\n",
    "test_docs = list(map(lambda i: names[i], split[1]))\n",
    "\n",
    "\n",
    "X_train = reduce(lambda l, dname: l + processed_documents[dname]['texts'], [[]] + train_docs)\n",
    "Y_train = reduce(lambda l, name: l + processed_documents[name]['labels'], [[]] + train_docs)\n",
    "\n",
    "\n",
    "X_test = reduce(lambda l, dname: l + processed_documents[dname]['texts'], [[]] + test_docs)\n",
    "Y_test = reduce(lambda l, name: l + processed_documents[name]['labels'], [[]] + test_docs)\n",
    "\n",
    "X = X_train + X_test\n",
    "Y = Y_train + Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text + Header\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Count Tokenizer\n",
    "all_texts = set()\n",
    "all_headers = set()\n",
    "for doc_name in processed_documents:\n",
    "    texts = processed_documents[doc_name]['texts']\n",
    "    labels = processed_documents[doc_name]['labels']\n",
    "    header_index = processed_documents[doc_name]['header_index']\n",
    "    \n",
    "    all_texts = all_texts | set(texts)\n",
    "    all_headers = all_headers | set([texts[i] for i in header_index if i != -1])\n",
    "\n",
    "all_texts = sorted(list(all_texts))\n",
    "all_headers = sorted(list(all_headers))\n",
    "\n",
    "text_index = {el:i for i, el in enumerate(all_texts)}\n",
    "header_index = {el:i for i, el in enumerate(all_headers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
    "all_texts = text_vectorizer.fit_transform(all_texts)\n",
    "\n",
    "header_vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
    "all_headers = header_vectorizer.fit_transform(all_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove data classes with insufficient examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_count = {el: Y.count(el) for el in set(Y)}\n",
    "\n",
    "min_count = 10\n",
    "X_train, Y_train = zip(*[(X_train[i], Y_train[i]) for i in range(len(X_train)) if Y_count[Y_train[i]] >= min_count])\n",
    "X_test, Y_test = zip(*[(X_test[i], Y_test[i]) for i in range(len(X_test)) if Y_count[Y_test[i]] >= min_count])\n",
    "\n",
    "X = X_train + X_test\n",
    "Y = Y_train + Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(Y))\n",
    "labels.sort(key=lambda x: Y.count(x))\n",
    "\n",
    "for l in labels:\n",
    "    print('-', l[:20], '| frequency:', Y.count(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline of feature engineering and model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = Pipeline([('vectorizer', CountVectorizer()),\n",
    " ('tfidf', TfidfTransformer()),\n",
    " ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced')))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramater selection\n",
    "# Params 1 #\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "               'tfidf__use_idf': [False, True],\n",
    "               'vectorizer__min_df': [0, 0.0001, 0.00001],\n",
    "               'vectorizer__stop_words':[None, 'english']\n",
    "             }\n",
    "gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X, Y)\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)\n",
    "\n",
    "# Save best found configuration\n",
    "save_value('best_params1', gs_clf_svm.best_params_, path=checkpoint_path)\n",
    "save_value('best_score1', gs_clf_svm.best_score_, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = load_value('best_params1', path=checkpoint_path)\n",
    "best_score = load_value('best_score1', path=checkpoint_path)\n",
    "\n",
    "#Training of Final Model\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,4), min_df = 0, stop_words=None)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "#Test\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(Y_test, pred)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(Y_test, pred)\n",
    "\n",
    "class_count = len(cm)\n",
    "\n",
    "class_accuracy = [cm[i][i]/sum(cm[i]) if sum(cm[i]) > 0 else 0 for i in range(class_count)]\n",
    "w_acc = [class_accuracy[i] * sum(cm[i]) for i in range(len(cm))]\n",
    "acc_no_other = (sum(w_acc) - w_acc[1])/(sum(sum(cm[i]) for i in range(class_count)) - sum(cm[1]))\n",
    "                                        \n",
    "print(cm)\n",
    "print('Class Acuracy:', class_accuracy)\n",
    "print('Overall Accuracy:', accuracy)\n",
    "print('Accuracy excluding other:', acc_no_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dict()\n",
    "for real, prediction in zip(Y_test, pred):\n",
    "    if real not in categories:\n",
    "        categories[real] = [0,0] #total, correct\n",
    "    categories[real][0] += 1\n",
    "    categories[real][1] += (real == prediction)\n",
    "    \n",
    "for c in categories:\n",
    "    tot = categories[c][0]\n",
    "    corr = categories[c][1]\n",
    "    acc = corr/tot\n",
    "#     print(c + ' =>', 'total:', tot, '\\t\\t correct:', corr, '\\t\\t accuracy:', acc)\n",
    "    print(c + ': acc', acc, 'total:', tot)\n",
    "#     print(c + ': correct:', corr, \"\\t || accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Search with Header augmented feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack, vstack\n",
    "from functools import reduce\n",
    "\n",
    "# Search Params\n",
    "ngram_configs = [(1, 1), (2, 2), (1, 2), (1, 3), (1, 4), (1, 5)]\n",
    "tfidf_configs = [True, False]\n",
    "vectorizer_stopwords_configs = ['english', None]\n",
    "min_df_configs = [0] + [10**(-n) for n in range(3, 4)]\n",
    "\n",
    "total_config_count = len(ngram_configs) * len(tfidf_configs) * len(vectorizer_stopwords_configs) * len(min_df_configs)\n",
    "\n",
    "# Vocab & Data Lists\n",
    "texts = set()\n",
    "headers = set()\n",
    "\n",
    "text_list = []\n",
    "label_list = []\n",
    "header1_list = []\n",
    "header2_list = []\n",
    "\n",
    "for doc_name in processed_documents:\n",
    "    doc = processed_documents[doc_name]\n",
    "    texts = texts | set(doc['texts'])\n",
    "    headers = headers | set(doc['texts'][i] for i in doc['header_index'] if i != -1)\n",
    "    \n",
    "    header1 = [doc['texts'][i] if i != -1 else \"\" for i in doc['header_index']]\n",
    "    header2 = [doc['texts'][i] if i != -1 else \"\" for i in doc['header_index_2']]\n",
    "    \n",
    "    doc['header1'] = header1\n",
    "    doc['header2'] = header2\n",
    "    \n",
    "    text_list.extend(doc['texts'])\n",
    "    label_list.extend(doc['labels'])\n",
    "    header1_list.extend(header1)\n",
    "    header2_list.extend(header2)\n",
    "\n",
    "save_value('processed_documents', processed_documents, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def multi_svm_train(doc_list, config=None):\n",
    "    train_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    train_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "    train_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "    train_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    if config is None:\n",
    "        config = load_value('best_config_header', path=checkpoint_path)\n",
    "        \n",
    "    text_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    header_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.fit_transform(train_texts)\n",
    "    tokenized_header1 = header_tokenizer.fit_transform(train_header1)\n",
    "    tokenized_header2 = header_tokenizer.transform(train_header2)\n",
    "    X_train = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    Y_train = train_labels\n",
    "\n",
    "    model = Pipeline([('tfidf', TfidfTransformer(use_idf=config['tfidf_config'])), ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return (model, text_tokenizer, header_tokenizer)\n",
    "\n",
    "def multi_svm_test(model, text_tokenizer, header_tokenizer, doc_list):\n",
    "    test_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    test_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "    test_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "    test_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.transform(test_texts)\n",
    "    tokenized_header1 = header_tokenizer.transform(test_header1)\n",
    "    tokenized_header2 = header_tokenizer.transform(test_header2)\n",
    "    X_test = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    Y_test = test_labels\n",
    "    pred = model.predict(X_test)\n",
    "    cm = np.array(confusion_matrix(Y_test, pred))\n",
    "    \n",
    "    # Diagonal elemetns were correctly classified\n",
    "    diagonal = cm.diagonal()\n",
    "    \n",
    "    # Input class Counts\n",
    "    class_sum = cm.sum(axis=1)\n",
    "    \n",
    "    # Predicted class counts\n",
    "    pred_sum = cm.sum(axis=0)\n",
    "    \n",
    "    # Per-class performance w/ no-examples -> 0 perf\n",
    "    precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
    "    recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n",
    "    \n",
    "    # Frequency Weighted Performance\n",
    "    c_freq = cm.sum(axis=1)/cm.sum()\n",
    "    pres = c_freq * precision\n",
    "    rec = c_freq * recall\n",
    "    \n",
    "    # Remove 'other' Category\n",
    "    c_freq = c_freq[0:1] + c_freq[2:]\n",
    "    pres = pres[0:1] + pres[2:]\n",
    "    rec = rec[0:1] + rec[2:]\n",
    "    \n",
    "    return pres.sum()/c_freq.sum(), rec.sum()/c_freq.sum()\n",
    "\n",
    "def cross_validation(doc_list, train_algo, test_algo, k, verbose=False, config=None):\n",
    "    N = len(doc_list)\n",
    "    size = N//k\n",
    "    indeces = list(range(N))\n",
    "    random.shuffle(indeces)\n",
    "    all_indeces = set(indeces)\n",
    "    \n",
    "    pres_list = []\n",
    "    rec_list = []\n",
    "    \n",
    "    for j in range(N//size):\n",
    "        train_indeces = indeces[j * size:(j + 1) * size] + indeces[size * k + j: size * k + j + 1]\n",
    "        test_indeces = list(all_indeces - set(train_indeces))\n",
    "        \n",
    "        train_docs = [doc_list[i] for i in train_indeces]\n",
    "        test_docs = [doc_list[i] for i in test_indeces]\n",
    "        \n",
    "        if verbose:\n",
    "            print('Fold %d starting!'%(j + 1))\n",
    "        \n",
    "        m, tt, ht = train_algo(train_docs, config=config)\n",
    "        pres, rec = test_algo(m, tt, ht, test_docs)\n",
    "        \n",
    "        pres_list.append(pres)\n",
    "        rec_list.append(rec)\n",
    "        \n",
    "        if verbose:\n",
    "            print('precision:', pres)\n",
    "            print('recall:', rec)\n",
    "            print('-' * 10 + '\\n')\n",
    "    \n",
    "    return sum(pres_list)/k, sum(rec_list)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "best_config = None\n",
    "best_f1 = -1\n",
    "\n",
    "count = 0\n",
    "for stop_config in vectorizer_stopwords_configs:\n",
    "    for ngram_config in ngram_configs:\n",
    "        for min_df_config in min_df_configs:\n",
    "            for tfidf_config in tfidf_configs:\n",
    "                count += 1\n",
    "                config = {\n",
    "                            'stop_config': stop_config,\n",
    "                            'ngram_config': ngram_config,\n",
    "                            'tfidf_config': tfidf_config,\n",
    "                            'min_df_config': min_df_config\n",
    "                         }\n",
    "                print('Progress: ' + str(count) + '/' + str(total_config_count), '\\t =>', count/total_config_count)\n",
    "                print('Testing configuration:', config)\n",
    "                pres, rec = cross_validation(list(processed_documents), multi_svm_train, multi_svm_test, 10, verbose=False)\n",
    "                f1 = 2 * (pres * rec)/(pres + rec)\n",
    "                \n",
    "                print('Precision: %f \\t Recall: %f, F1: %f'%(pres, rec, f1))\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_config = config\n",
    "                    best_f1 = f1\n",
    "\n",
    "\n",
    "save_value('best_config_header', config, path=checkpoint_path)\n",
    "save_value('best_score_header', f1, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_value('best_config_header', path=checkpoint_path)\n",
    "acc = load_value('best_score_header', path=checkpoint_path)\n",
    "print(config)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_value('best_config_header', path=checkpoint_path)\n",
    "\n",
    "avg_precision, avg_recall = cross_validation(list(processed_documents), multi_svm_train, multi_svm_test, 10, verbose=True, config=config)\n",
    "print('Average Precision:', avg_precision)\n",
    "print('Average Recall:', avg_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_value('best_header_config', path=checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
