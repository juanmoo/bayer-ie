{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "import utils, json\n",
    "from tqdm import tqdm\n",
    "from utils import save_value, load_value, load_env_keys, match_labels, tokenize_string, clean_matches\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '/scratch/juanmoo1'\n",
    "shared_path = os.path.join(data_path, './shared')\n",
    "\n",
    "EMA_dump_path = os.path.join(data_path, './jsons/new_EMA_dump.json')\n",
    "EMA_old_dump_path = os.path.join(data_path, './jsons/EMA_dump.json')\n",
    "\n",
    "EMA_annotations_path = os.path.join(data_path, './bayer/VendorEMAforMIT/new_annotations/annotations.xlsx')\n",
    "EMA_old_annotations_path = os.path.join(data_path, './bayer/VendorEMAforMIT/annotations.xlsx')\n",
    "\n",
    "pickle_dumps_path = os.path.join(data_path, './pickle_dumps/')\n",
    "checkpoint_path = os.path.join(pickle_dumps_path, 'checkpoint.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data\n",
    "'''\n",
    "Format:\n",
    "{\n",
    "    document_name <str>: {\n",
    "                            element_text: <str> (raw text),\n",
    "                            element_tag: <str> (TEI XML tag)\n",
    "                          },\n",
    "                          \n",
    "    ...\n",
    "}\n",
    "'''\n",
    "raw_data = json.loads(open(EMA_dump_path, 'r').read())\n",
    "old_raw_data = json.loads(open(EMA_old_dump_path, 'r').read())\n",
    "\n",
    "\n",
    "# Labels\n",
    "'''\n",
    "Dict in form:\n",
    "{\n",
    "    file_name: {\n",
    "        texts: [ <str>, ...],\n",
    "        labels: [ <str>, ...]\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "}\n",
    "'''\n",
    "annotations = utils.parse_spreadsheet(EMA_annotations_path)\n",
    "old_annotations = utils.parse_spreadsheet(EMA_old_annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Data to Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 196.34it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Iterates through each document in the dataset and compares is to labels with the same file name. Matching is done using fuzzy string matching unless the exact_matching is set to True.\n",
    "'''\n",
    "\n",
    "labeled_raw_documents = match_labels(raw_data, annotations, exact_match=True)\n",
    "# save_value('labeled_raw_documents', labeled_raw_documents, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<00:00, 554.94it/s]\n"
     ]
    }
   ],
   "source": [
    "old_labeled_raw_documents = match_labels(old_raw_data, old_annotations, exact_match=True)\n",
    "# save_value('old_raw_labeled_documents', old_labeled_raw_documents, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_documents = load_value('labeled_raw_documents', path=checkpoint_path)\n",
    "old_labeled_raw_documents = load_value('old_labeled_raw_documents', path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Clean input text\n",
    "data = clean_matches(labeled_raw_documents)\n",
    "old_data = clean_matches(old_labeled_raw_documents)\n",
    "\n",
    "# One-hot encode labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "old_data = old_data.join(pd.DataFrame(mlb.fit_transform(old_data['label']), columns=mlb.classes_, index=old_data.index))\n",
    "data = data.join(pd.DataFrame(mlb.transform(data['label']), columns=mlb.classes_, index=data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline of feature engineering and model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = Pipeline([('vectorizer', CountVectorizer()),\n",
    " ('tfidf', TfidfTransformer()),\n",
    " ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced')))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Search with Header augmented feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack, vstack\n",
    "from functools import reduce\n",
    "\n",
    "# Search Params\n",
    "ngram_configs = [(1, 1), (2, 2), (1, 2), (1, 3), (1, 4), (1, 5)]\n",
    "tfidf_configs = [True, False]\n",
    "vectorizer_stopwords_configs = ['english', None]\n",
    "min_df_configs = [0] + [10**(-n) for n in range(3, 4)]\n",
    "\n",
    "total_config_count = len(ngram_configs) * len(tfidf_configs) * len(vectorizer_stopwords_configs) * len(min_df_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def multi_svm_train(doc_list, config=None):\n",
    "    train_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    train_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "    train_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "    train_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    if config is None:\n",
    "        config = load_value('best_config_header', path=checkpoint_path)\n",
    "        \n",
    "    text_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    header_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.fit_transform(train_texts)\n",
    "    tokenized_header1 = header_tokenizer.fit_transform(train_header1)\n",
    "    tokenized_header2 = header_tokenizer.transform(train_header2)\n",
    "    X_train = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    Y_train = train_labels\n",
    "\n",
    "    model = Pipeline([('tfidf', TfidfTransformer(use_idf=config['tfidf_config'])), ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return (model, text_tokenizer, header_tokenizer)\n",
    "\n",
    "def multi_svm_test(model, text_tokenizer, header_tokenizer, doc_list):\n",
    "    test_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    test_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "    test_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "    test_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.transform(test_texts)\n",
    "    tokenized_header1 = header_tokenizer.transform(test_header1)\n",
    "    tokenized_header2 = header_tokenizer.transform(test_header2)\n",
    "    X_test = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    Y_test = test_labels\n",
    "    pred = model.predict(X_test)\n",
    "    cm = np.array(confusion_matrix(Y_test, pred))\n",
    "    \n",
    "    # Diagonal elemetns were correctly classified\n",
    "    diagonal = cm.diagonal()\n",
    "    \n",
    "    # Input class Counts\n",
    "    class_sum = cm.sum(axis=1)\n",
    "    \n",
    "    # Predicted class counts\n",
    "    pred_sum = cm.sum(axis=0)\n",
    "    \n",
    "    # Per-class performance w/ no-examples -> 0 perf\n",
    "    precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
    "    recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n",
    "    \n",
    "    # Frequency Weighted Performance\n",
    "    c_freq = cm.sum(axis=1)/cm.sum()\n",
    "    pres = c_freq * precision\n",
    "    rec = c_freq * recall\n",
    "    \n",
    "    # Remove 'other' Category\n",
    "    c_freq = c_freq[0:1] + c_freq[2:]\n",
    "    pres = pres[0:1] + pres[2:]\n",
    "    rec = rec[0:1] + rec[2:]\n",
    "    \n",
    "    return pres.sum()/c_freq.sum(), rec.sum()/c_freq.sum()\n",
    "\n",
    "def cross_validation(doc_list, train_algo, test_algo, k, verbose=False, config=None):\n",
    "    N = len(doc_list)\n",
    "    size = N//k\n",
    "    indeces = list(range(N))\n",
    "    random.shuffle(indeces)\n",
    "    all_indeces = set(indeces)\n",
    "    \n",
    "    pres_list = []\n",
    "    rec_list = []\n",
    "    \n",
    "    for j in range(N//size):\n",
    "        train_indeces = indeces[j * size:(j + 1) * size] + indeces[size * k + j: size * k + j + 1]\n",
    "        test_indeces = list(all_indeces - set(train_indeces))\n",
    "        \n",
    "        train_docs = [doc_list[i] for i in train_indeces]\n",
    "        test_docs = [doc_list[i] for i in test_indeces]\n",
    "        \n",
    "        if verbose:\n",
    "            print('Fold %d starting!'%(j + 1))\n",
    "        \n",
    "        m, tt, ht = train_algo(train_docs, config=config)\n",
    "        pres, rec = test_algo(m, tt, ht, test_docs)\n",
    "        \n",
    "        pres_list.append(pres)\n",
    "        rec_list.append(rec)\n",
    "        \n",
    "        if verbose:\n",
    "            print('precision:', pres)\n",
    "            print('recall:', rec)\n",
    "            print('-' * 10 + '\\n')\n",
    "    \n",
    "    return sum(pres_list)/k, sum(rec_list)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "best_config = None\n",
    "best_f1 = -1\n",
    "\n",
    "count = 0\n",
    "for stop_config in vectorizer_stopwords_configs:\n",
    "    for ngram_config in ngram_configs:\n",
    "        for min_df_config in min_df_configs:\n",
    "            for tfidf_config in tfidf_configs:\n",
    "                count += 1\n",
    "                config = {\n",
    "                            'stop_config': stop_config,\n",
    "                            'ngram_config': ngram_config,\n",
    "                            'tfidf_config': tfidf_config,\n",
    "                            'min_df_config': min_df_config\n",
    "                         }\n",
    "                print('Progress: ' + str(count) + '/' + str(total_config_count), '\\t =>', count/total_config_count)\n",
    "                print('Testing configuration:', config)\n",
    "                pres, rec = cross_validation(list(processed_documents), multi_svm_train, multi_svm_test, 10, verbose=False)\n",
    "                f1 = 2 * (pres * rec)/(pres + rec)\n",
    "                \n",
    "                print('Precision: %f \\t Recall: %f, F1: %f'%(pres, rec, f1))\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_config = config\n",
    "                    best_f1 = f1\n",
    "\n",
    "\n",
    "save_value('best_config_header', config, path=checkpoint_path)\n",
    "save_value('best_score_header', f1, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_value('best_config_header', path=checkpoint_path)\n",
    "acc = load_value('best_score_header', path=checkpoint_path)\n",
    "print(config)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_value('best_config_header', path=checkpoint_path)\n",
    "\n",
    "avg_precision, avg_recall = cross_validation(list(processed_documents), multi_svm_train, multi_svm_test, 10, verbose=True, config=config)\n",
    "print('Average Precision:', avg_precision)\n",
    "print('Average Recall:', avg_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_value('best_header_config', path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Concept Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack, vstack\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def svm_train(train_docs, data, label, config=None):\n",
    "    train_data = data.loc[data['document'].isin(train_docs)]\n",
    "    \n",
    "    if config is None:\n",
    "        config = {\n",
    "            'ngram_config': (1, 4),\n",
    "            'stop_config': 'english',\n",
    "            'tfidf_config': True\n",
    "        }\n",
    "        \n",
    "    text_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    head1_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    head2_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.fit_transform(train_data['text'])\n",
    "    tokenized_head1 = head1_tokenizer.fit_transform(train_data['head1'])\n",
    "    tokenized_head2 = head2_tokenizer.fit_transform(train_data['head2'])\n",
    "    \n",
    "    X_train = hstack([tokenized_texts, tokenized_head1, tokenized_head2])\n",
    "    Y_train = train_data[label]\n",
    "    \n",
    "    model = Pipeline([('tfidf', TfidfTransformer(use_idf=config['tfidf_config'])), ('clf', LinearSVC(class_weight=\"balanced\"))])\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'tt': text_tokenizer,\n",
    "        'h1t': head1_tokenizer,\n",
    "        'h2t': head2_tokenizer,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "def svm_test(test_docs, data, params):\n",
    "    \n",
    "    test_data = data.loc[data['document'].isin(test_docs)]\n",
    "    \n",
    "    tt = params['tt'].transform(test_data['text'])\n",
    "    th1 = params['h1t'].transform(test_data['head1'])\n",
    "    th2 = params['h2t'].transform(test_data['head2'])\n",
    "    \n",
    "    X_test = hstack([tt, th1, th2])\n",
    "    Y_test = np.array((test_data[params['label']])).reshape(-1, 1) * 1.0\n",
    "    \n",
    "    \n",
    "    pred = np.array(params['model'].predict(X_test)).reshape(-1, 1) * 1.0\n",
    "    cm = np.array(confusion_matrix(Y_test, pred))\n",
    "    \n",
    "    \n",
    "    # Diagonal elemetns were correctly classified\n",
    "    diagonal = cm.diagonal()\n",
    "    \n",
    "    # Input class Counts\n",
    "    class_sum = cm.sum(axis=1)\n",
    "    \n",
    "    # Predicted class counts\n",
    "    pred_sum = cm.sum(axis=0)\n",
    "    \n",
    "    # Per-class performance w/ no-examples -> 0 perf\n",
    "    precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
    "    recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n",
    "    \n",
    "    # Frequency Weighted Performance\n",
    "    c_freq = cm.sum(axis=1)/cm.sum()\n",
    "    pres = c_freq * precision\n",
    "    rec = c_freq * recall\n",
    "    \n",
    "    # Remove 'other' Category\n",
    "    c_freq = c_freq[1:]\n",
    "    pres = pres[1:] \n",
    "    rec = rec[1:]\n",
    "    \n",
    "    output = {\n",
    "        'precision': pres.sum()/c_freq.sum(),\n",
    "        'recall': rec.sum()/c_freq.sum(),\n",
    "        'cm': cm,\n",
    "        'all_predicted': test_data.loc[pred > 0][['head2', 'head1', 'text']],\n",
    "        'actual_positive': test_data.loc[Y_test > 0][['head2', 'head1', 'text']],\n",
    "        'true_positive': test_data.loc[Y_test * pred > 0][['head2', 'head1', 'text']],\n",
    "        'false_positive': test_data.loc[pred * (1 - Y_test) > 0][['head2', 'head1', 'text']],\n",
    "        'false_negative': test_data.loc[Y_test * (1 - pred) > 0][['head2', 'head1', 'text']]\n",
    "    }\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(mlb.classes_)\n",
    "documents = pd.unique(data['document'])\n",
    "train_docs = documents[:3]\n",
    "test_docs = documents[3:]\n",
    "\n",
    "data_train = data.loc[data['document'].isin(train_docs)]\n",
    "data_test = data.loc[data['document'].isin(test_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Testing Label: Contraindication ====================\n",
      "Confussion Matrix: \n",
      "[[163   0]\n",
      " [  1   2]]\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 1.0\n",
      "F1: 0.8\n",
      "Training Examples Count: 4\n",
      "Test Examples Count: 3\n",
      "\n",
      "==================== Testing Label: Populations - Adolescent ====================\n",
      "Confussion Matrix: \n",
      "[[157   3]\n",
      " [  4   2]]\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.4\n",
      "F1: 0.3636363636363636\n",
      "Training Examples Count: 33\n",
      "Test Examples Count: 6\n",
      "\n",
      "==================== Testing Label: Populations - Adult ====================\n",
      "Confussion Matrix: \n",
      "[[165   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "Training Examples Count: 40\n",
      "Test Examples Count: 1\n",
      "\n",
      "==================== Testing Label: Populations - Geriatric ====================\n",
      "Confussion Matrix: \n",
      "[[163   0]\n",
      " [  3   0]]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "Training Examples Count: 16\n",
      "Test Examples Count: 3\n",
      "\n",
      "==================== Testing Label: Populations - Paediatric ====================\n",
      "Confussion Matrix: \n",
      "[[154   3]\n",
      " [  6   3]]\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.5\n",
      "F1: 0.4\n",
      "Training Examples Count: 35\n",
      "Test Examples Count: 9\n",
      "\n",
      "==================== Testing Label: Significant Findings - Fertility ====================\n",
      "There were only 0training examples. 2 or more are needed to train the model.\n",
      "\n",
      "==================== Testing Label: Significant Findings - Hepatic Impairment ====================\n",
      "Confussion Matrix: \n",
      "[[166]]\n",
      "Precision: nan\n",
      "Recall: nan\n",
      "F1: nan\n",
      "Training Examples Count: 2\n",
      "Test Examples Count: 0\n",
      "\n",
      "==================== Testing Label: Significant Findings - Lactation ====================\n",
      "There were only 0training examples. 2 or more are needed to train the model.\n",
      "\n",
      "==================== Testing Label: Significant Findings - Pregnancy ====================\n",
      "Confussion Matrix: \n",
      "[[163   1]\n",
      " [  2   0]]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "Training Examples Count: 5\n",
      "Test Examples Count: 2\n",
      "\n",
      "==================== Testing Label: Significant Findings - Renal Impairment ====================\n",
      "There were only 1training examples. 2 or more are needed to train the model.\n",
      "\n",
      "==================== Testing Label: Warning ====================\n",
      "Confussion Matrix: \n",
      "[[143   2]\n",
      " [ 11  10]]\n",
      "Precision: 0.47619047619047616\n",
      "Recall: 0.8333333333333334\n",
      "F1: 0.6060606060606061\n",
      "Training Examples Count: 67\n",
      "Test Examples Count: 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Ignore division by zero when calculating F1 score\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning) \n",
    "output_file = os.path.join(shared_path, 'trainNew_testNew.txt')\n",
    "\n",
    "with open(output_file, 'w') as outFile:\n",
    "    \n",
    "    for l in labels:\n",
    "        if l != 'other':\n",
    "            summary = '=' * 20 + ' Testing Label: ' + str(l) + ' ' + '=' * 20 + '\\n'\n",
    "            out = ''\n",
    "            \n",
    "\n",
    "            train_count = data_train[l].sum()\n",
    "            test_count = data_test[l].sum()\n",
    "                        \n",
    "\n",
    "            if train_count > 1:\n",
    "                \n",
    "                params = svm_train(train_docs, data, l)\n",
    "                output = svm_test(test_docs, data, params)\n",
    "\n",
    "                precision = output['precision']\n",
    "                recall = output['recall']\n",
    "                cm = output['cm']             \n",
    "\n",
    "                all_predicted = output['all_predicted']\n",
    "                actual_positive = output['actual_positive']\n",
    "                true_positive = output['true_positive']\n",
    "                false_positive = output['false_positive']\n",
    "                false_negative = output['false_negative']\n",
    "\n",
    "\n",
    "                summary += 'Confussion Matrix: \\n'\n",
    "                summary += str(cm) + '\\n'\n",
    "                \n",
    "                \n",
    "                \n",
    "                summary += 'Precision: ' + str(precision) + '\\n'\n",
    "                summary += 'Recall: ' + str(recall) + '\\n'\n",
    "                summary += 'F1: ' + str(2 * (precision * recall)/(precision + recall)) + '\\n'\n",
    "\n",
    "\n",
    "                summary += 'Training Examples Count: ' + str(train_count) + '\\n'\n",
    "                summary += 'Test Examples Count: ' + str(test_count) + '\\n'\n",
    "                \n",
    "                example_head = '-' * 20 + ' %s ' + '-' * 20 + '\\n'\n",
    "                example_format = '# %d. HEAD2: %s \\nHEAD1: %s\\nTEXT: %s\\n\\n\\n'\n",
    "                \n",
    "                \n",
    "                out += example_head%('PREDICTED')\n",
    "                for index, (h2, h1, t) in all_predicted.iterrows():\n",
    "                    out += example_format%(index, h2, h1, t)\n",
    "                out += '\\n'\n",
    "                \n",
    "                out += example_head%('TRUE POSITIVE')\n",
    "                for index, (h2, h1, t) in true_positive.iterrows():\n",
    "                    out += example_format%(index, h2, h1, t)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE NEGATIVE')\n",
    "                for index, (h2, h1, t) in false_negative.iterrows():\n",
    "                    out += example_format%(index, h2, h1, t)\n",
    "                out += '\\n'\n",
    "                    \n",
    "                out += example_head%('FALSE POSITIVE')\n",
    "                for index, (h2, h1, t) in false_positive.iterrows():\n",
    "                    out += example_format%(index, h2, h1, t)\n",
    "                out += '\\n'\n",
    "                \n",
    "            else:\n",
    "                summary += 'There were only ' + str(train_count) + 'training examples. 2 or more are needed to train the model.'\n",
    "                summary += '\\n'\n",
    "            \n",
    "            print(summary)\n",
    "            outFile.write(summary + '\\n')\n",
    "            outFile.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on old, test in new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.unique(old_data['label'])\n",
    "train_docs2 = pd.unique(old_data['document'])\n",
    "# test_docs are the samve as above\n",
    "\n",
    "data_train = data.loc[data['document'].isin(train_docs2)]\n",
    "data_test = data.loc[data['document'].isin(test_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Testing Label: Contraindication ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171   0]\n",
      " [  1   2]]\n",
      "\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 1.0\n",
      "F1: 0.8\n",
      "\n",
      "Training Examples Count: 9\n",
      "Test Examples Count: 3\n",
      "\n",
      "========== Testing Label: Warning ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[146   3]\n",
      " [  5  20]]\n",
      "\n",
      "Precision: 0.8\n",
      "Recall: 0.8695652173913043\n",
      "F1: 0.8333333333333333\n",
      "\n",
      "Training Examples Count: 105\n",
      "Test Examples Count: 25\n",
      "\n",
      "========== Testing Label: Significant Findings - Pregnancy ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[167   4]\n",
      " [  0   3]]\n",
      "\n",
      "Precision: 1.0\n",
      "Recall: 0.42857142857142855\n",
      "F1: 0.6\n",
      "\n",
      "Training Examples Count: 17\n",
      "Test Examples Count: 3\n",
      "\n",
      "========== Testing Label: Populations - Paediatric ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[161   4]\n",
      " [  7   2]]\n",
      "\n",
      "Precision: 0.2222222222222222\n",
      "Recall: 0.3333333333333333\n",
      "F1: 0.26666666666666666\n",
      "\n",
      "Training Examples Count: 42\n",
      "Test Examples Count: 9\n",
      "\n",
      "========== Testing Label: Populations - Geriatric ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171   0]\n",
      " [  3   0]]\n",
      "\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "\n",
      "Training Examples Count: 21\n",
      "Test Examples Count: 3\n",
      "\n",
      "========== Testing Label: Significant Findings - Hepatic Impairment ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[174]]\n",
      "\n",
      "Precision: nan\n",
      "Recall: nan\n",
      "F1: nan\n",
      "\n",
      "Training Examples Count: 3\n",
      "Test Examples Count: 0\n",
      "\n",
      "========== Testing Label: Significant Findings - Renal Impairment ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171   0]\n",
      " [  3   0]]\n",
      "\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "\n",
      "Training Examples Count: 5\n",
      "Test Examples Count: 3\n",
      "\n",
      "========== Testing Label: Populations - Adult ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[172   0]\n",
      " [  2   0]]\n",
      "\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n",
      "\n",
      "Training Examples Count: 45\n",
      "Test Examples Count: 2\n",
      "\n",
      "========== Testing Label: Populations - Adolescent ==========\n",
      "\n",
      "Confusion Matrix:\n",
      "[[163   6]\n",
      " [  4   1]]\n",
      "\n",
      "Precision: 0.2\n",
      "Recall: 0.14285714285714285\n",
      "F1: 0.16666666666666666\n",
      "\n",
      "Training Examples Count: 36\n",
      "Test Examples Count: 5\n",
      "\n",
      "========== Testing Label: Significant Findings - Lactation ==========\n",
      "\n",
      "There were only 0 training examples. 2 or more are needed to train the model.\n",
      "\n",
      "========== Testing Label: Significant Findings - Fertility ==========\n",
      "\n",
      "There were only 0 training examples. 2 or more are needed to train the model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Ignore division by zero when calculating F1 score\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning) \n",
    "\n",
    "for l in labels:\n",
    "    if l != 'other':\n",
    "        print('=' * 10 , 'Testing Label:', l, '=' * 10)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        train_count = (data_train['label'] == l).sum()\n",
    "        test_count = (data_test['label'] == l).sum()\n",
    "        \n",
    "        if train_count > 1:\n",
    "            \n",
    "            params = svm_train(train_docs, data, l)\n",
    "            precision, recall, cm = svm_test(test_docs, data, params)\n",
    "\n",
    "            print('Confusion Matrix:')\n",
    "            print(cm)\n",
    "            print()\n",
    "\n",
    "            print('Precision:', precision)\n",
    "            print('Recall:', recall)\n",
    "            print('F1:', 2 * (precision * recall)/(precision + recall))\n",
    "            print()\n",
    "\n",
    "            print('Training Examples Count:', train_count)\n",
    "            print('Test Examples Count:', test_count)\n",
    "            print()   \n",
    "        else:\n",
    "            \n",
    "            print('There were only', train_count, 'training examples. 2 or more are needed to train the model.')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
