{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "import json\n",
    "from utils import save_value, load_value, load_env_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '/scratch/juanmoo1'\n",
    "EMA_dump_path = os.path.join(data_path, './jsons/EMA_dump.json')\n",
    "EMA_xmls_path = os.path.join(data_path, './xmls/')\n",
    "EMA_annotations_path = os.path.join(data_path, './bayer/VendorEMAforMIT/annotations.xlsx')\n",
    "\n",
    "pickle_dumps_path = os.path.join(data_path, './pickle_dumps/')\n",
    "checkpoint_file = os.path.normpath(os.path.join(pickle_dumps_path, 'checkpoint.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_documents = load_value('processed_documents', path=checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Procedures for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def cross_validation(doc_list, train_algo, test_algo, k, verbose=False, config=None):\n",
    "    N = len(doc_list)\n",
    "    size = N//k\n",
    "    indeces = list(range(N))\n",
    "    random.shuffle(indeces)\n",
    "    all_indeces = set(indeces)\n",
    "    \n",
    "    pres_list = []\n",
    "    rec_list = []\n",
    "    \n",
    "    for j in range(N//size):\n",
    "        train_indeces = indeces[j * size:(j + 1) * size] + indeces[size * k + j: size * k + j + 1]\n",
    "        test_indeces = list(all_indeces - set(train_indeces))\n",
    "        \n",
    "        train_docs = [doc_list[i] for i in train_indeces]\n",
    "        test_docs = [doc_list[i] for i in test_indeces]\n",
    "        \n",
    "        if verbose:\n",
    "            print('Fold %d starting!'%(j + 1))\n",
    "        \n",
    "        m, tt, ht = train_algo(train_docs, config=config)\n",
    "        pres, rec = test_algo(m, tt, ht, test_docs)\n",
    "        \n",
    "        pres_list.append(pres)\n",
    "        rec_list.append(rec)\n",
    "        \n",
    "        if verbose:\n",
    "            print('precision:', pres)\n",
    "            print('recall:', rec)\n",
    "            print('-' * 10 + '\\n')\n",
    "    \n",
    "    return sum(pres_list)/k, sum(rec_list)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, vstack\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def model_test(model, text_tokenizer, header_tokenizer, doc_list, use_headers = True):\n",
    "    test_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    tokenized_texts = text_tokenizer.transform(test_texts)\n",
    "    test_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    if use_headers:\n",
    "        test_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "        test_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "        tokenized_header1 = header_tokenizer.transform(test_header1)\n",
    "        tokenized_header2 = header_tokenizer.transform(test_header2)\n",
    "        X_test = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    else:\n",
    "        X_test = tokenized_texts\n",
    "        \n",
    "    Y_test = test_labels\n",
    "    pred = model.predict(X_test)\n",
    "    cm = np.array(confusion_matrix(Y_test, pred))\n",
    "    \n",
    "    # Diagonal elemetns were correctly classified\n",
    "    diagonal = cm.diagonal()\n",
    "    \n",
    "    # Input class Counts\n",
    "    class_sum = cm.sum(axis=1)\n",
    "    \n",
    "    # Predicted class counts\n",
    "    pred_sum = cm.sum(axis=0)\n",
    "    \n",
    "    # Per-class performance w/ no-examples -> 0 perf\n",
    "    precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
    "    recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n",
    "    \n",
    "    # Frequency Weighted Performance\n",
    "    c_freq = cm.sum(axis=1)/cm.sum()\n",
    "    pres = c_freq * precision\n",
    "    rec = c_freq * recall\n",
    "    \n",
    "    # Remove 'other' Category\n",
    "    c_freq = c_freq[0:1] + c_freq[2:]\n",
    "    pres = pres[0:1] + pres[2:]\n",
    "    rec = rec[0:1] + rec[2:]\n",
    "    \n",
    "    return pres.sum()/c_freq.sum(), rec.sum()/c_freq.sum(), cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def mlp_train(doc_list, config=None, use_headers=True):\n",
    "    train_texts = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['texts'] for doc_name in doc_list])\n",
    "    train_labels = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['labels'] for doc_name in doc_list])\n",
    "    \n",
    "    if use_headers:\n",
    "        train_header1 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header1'] for doc_name in doc_list])\n",
    "        train_header2 = reduce(lambda x, y: x + y, [[]] + [processed_documents[doc_name]['header2'] for doc_name in doc_list])\n",
    "\n",
    "    if config is None:\n",
    "        config = load_value('best_header_config', path=checkpoint_file)\n",
    "        \n",
    "    text_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "    \n",
    "    if use_headers:\n",
    "        header_tokenizer = CountVectorizer(ngram_range=config['ngram_config'], stop_words=config['stop_config'])\n",
    "        tokenized_header1 = header_tokenizer.fit_transform(train_header1)\n",
    "        tokenized_header2 = header_tokenizer.transform(train_header2)\n",
    "    else:\n",
    "        header_tokenizer = None\n",
    "\n",
    "    \n",
    "    tokenized_texts = text_tokenizer.fit_transform(train_texts)\n",
    "    \n",
    "    if use_headers:\n",
    "        X_train = hstack([tokenized_texts, tokenized_header1, tokenized_header2])\n",
    "    else:\n",
    "        X_train = tokenized_texts\n",
    "        \n",
    "        \n",
    "    Y_train = train_labels\n",
    "    \n",
    "    model = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100, 100), random_state=None, verbose=True)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return (model, text_tokenizer, header_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, tt, ht = mlp_train(list(processed_documents), config=None)\n",
    "# Save Model\n",
    "save_value('neural_model_100_100', path=checkpoint_file)\n",
    "save_value('neural_model_tt_100_100', tt, path=checkpoint_file)\n",
    "save_value('neural_model_ht_100_100', ht, path=checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = load_value('neural_model_100_100', path=checkpoint_file)\n",
    "tt = load_value('neural_model_tt_100_100', path=checkpoint_file)\n",
    "ht = load_value('neural_model_ht_100_100', path=checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f44fb36e900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neural_model_ht_100_100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-186f5d7ec9d4>\u001b[0m in \u001b[0;36mmodel_test\u001b[0;34m(model, text_tokenizer, header_tokenizer, doc_list, use_headers)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocessed_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokenized_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocessed_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "p, r, cm = model_test(m, tt, ht, list(processed_documents))\n",
    "print('Precision:', p)\n",
    "print('Recall:', r)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80/20 Train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "import random\n",
    "\n",
    "names = list(processed_documents)\n",
    "random.shuffle(names)\n",
    "\n",
    "i = int(.8 * len(names))\n",
    "train_docs = names[:i]\n",
    "test_docs = names[i:]\n",
    "\n",
    "# Save Used Split\n",
    "save_value('8020_split_train', train_docs, path=checkpoint_file)\n",
    "save_value('8020_split_test', test_docs, path=checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53449386\n",
      "Iteration 2, loss = 0.11339391\n",
      "Iteration 3, loss = 0.06942646\n",
      "Iteration 4, loss = 0.05202540\n",
      "Iteration 5, loss = 0.04540920\n",
      "Iteration 6, loss = 0.04124154\n",
      "Iteration 7, loss = 0.03727206\n",
      "Iteration 8, loss = 0.03557919\n",
      "Iteration 9, loss = 0.03419705\n",
      "Iteration 10, loss = 0.03378738\n",
      "Iteration 11, loss = 0.03323914\n",
      "Iteration 12, loss = 0.03285407\n",
      "Iteration 13, loss = 0.03114829\n",
      "Iteration 14, loss = 0.03106815\n",
      "Iteration 15, loss = 0.03023612\n",
      "Iteration 16, loss = 0.03095014\n",
      "Iteration 17, loss = 0.03062793\n",
      "Iteration 18, loss = 0.03030639\n",
      "Iteration 19, loss = 0.03057404\n",
      "Iteration 20, loss = 0.02969658\n",
      "Iteration 21, loss = 0.02933685\n",
      "Iteration 22, loss = 0.02931908\n",
      "Iteration 23, loss = 0.02899157\n",
      "Iteration 24, loss = 0.02892000\n",
      "Iteration 25, loss = 0.02909354\n",
      "Iteration 26, loss = 0.02836927\n",
      "Iteration 27, loss = 0.02849505\n",
      "Iteration 28, loss = 0.02862916\n",
      "Iteration 29, loss = 0.02800112\n",
      "Iteration 30, loss = 0.02859718\n",
      "Iteration 31, loss = 0.02795442\n",
      "Iteration 32, loss = 0.02753297\n",
      "Iteration 33, loss = 0.02778565\n",
      "Iteration 34, loss = 0.02796179\n",
      "Iteration 35, loss = 0.02781949\n",
      "Iteration 36, loss = 0.02781646\n",
      "Iteration 37, loss = 0.02726256\n",
      "Iteration 38, loss = 0.02783352\n",
      "Iteration 39, loss = 0.02757564\n",
      "Iteration 40, loss = 0.02789747\n",
      "Iteration 41, loss = 0.02735078\n",
      "Iteration 42, loss = 0.02736049\n",
      "Iteration 43, loss = 0.02722810\n",
      "Iteration 44, loss = 0.02714784\n",
      "Iteration 45, loss = 0.02735290\n",
      "Iteration 46, loss = 0.02745008\n",
      "Iteration 47, loss = 0.02728268\n",
      "Iteration 48, loss = 0.02714440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "m2, tt2, ht2 = mlp_train(train_docs, config=None, use_headers=True)\n",
    "\n",
    "# Save Model\n",
    "save_value('neural_model_100_100_8020', m2, path=checkpoint_file)\n",
    "save_value('neural_model_tt_100_100_8020', tt2, path=checkpoint_file)\n",
    "save_value('neural_model_ht_100_100_8020', ht2, path=checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4799999999999999\n",
      "Recall: 0.2781074979650294\n",
      "[[  47   15    0    0    0    0    0    0    0    7    0    9]\n",
      " [  99 5887    7    6    0   13    0    1    0   32    0  128]\n",
      " [   0    3    0    0    0    1    0    0    0    0    0    0]\n",
      " [   0    1    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0   19    0    0    0    0    0    0    0    0    0    2]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    1    0    0]\n",
      " [   1   12    0    0    0    0    0    0    0    0    0    5]\n",
      " [   0    1    0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   64    0    0    0    1    0    0    0   33    0    4]\n",
      " [   0    8    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4  124    0    0    0    0    0    2    0    7    0   25]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-186f5d7ec9d4>:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "  precision = np.where(class_sum == 0, 0, diagonal/class_sum)\n",
      "<ipython-input-22-186f5d7ec9d4>:35: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n"
     ]
    }
   ],
   "source": [
    "m2 = load_value('neural_model_100_100_8020', path=checkpoint_file)\n",
    "tt2 = load_value('neural_model_tt_100_100_8020', path=checkpoint_file)\n",
    "ht2 = load_value('neural_model_ht_100_100_8020', path=checkpoint_file)\n",
    "test_docs = load_value('8020_split_test', path=checkpoint_file)\n",
    "\n",
    "# Model Test\n",
    "p, r, cm = model_test(m2, tt2, ht2, test_docs)\n",
    "print('Precision:', p)\n",
    "print('Recall:', r)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84406257\n",
      "Iteration 2, loss = 0.15876564\n",
      "Iteration 3, loss = 0.10261932\n",
      "Iteration 4, loss = 0.08203058\n",
      "Iteration 5, loss = 0.07047248\n",
      "Iteration 6, loss = 0.06466662\n",
      "Iteration 7, loss = 0.06038527\n",
      "Iteration 8, loss = 0.05784284\n",
      "Iteration 9, loss = 0.05684197\n",
      "Iteration 10, loss = 0.05629444\n",
      "Iteration 11, loss = 0.05483570\n",
      "Iteration 12, loss = 0.05396532\n",
      "Iteration 13, loss = 0.05399739\n",
      "Iteration 14, loss = 0.05299765\n",
      "Iteration 15, loss = 0.05296739\n",
      "Iteration 16, loss = 0.05283849\n",
      "Iteration 17, loss = 0.05256334\n",
      "Iteration 18, loss = 0.05262052\n",
      "Iteration 19, loss = 0.05186606\n",
      "Iteration 20, loss = 0.05175251\n",
      "Iteration 21, loss = 0.05237662\n",
      "Iteration 22, loss = 0.05190384\n",
      "Iteration 23, loss = 0.05168975\n",
      "Iteration 24, loss = 0.05156262\n",
      "Iteration 25, loss = 0.05153554\n",
      "Iteration 26, loss = 0.05160857\n",
      "Iteration 27, loss = 0.05218182\n",
      "Iteration 28, loss = 0.05098553\n",
      "Iteration 29, loss = 0.05227251\n",
      "Iteration 30, loss = 0.05169069\n",
      "Iteration 31, loss = 0.05092994\n",
      "Iteration 32, loss = 0.05114451\n",
      "Iteration 33, loss = 0.05152872\n",
      "Iteration 34, loss = 0.05101697\n",
      "Iteration 35, loss = 0.05110003\n",
      "Iteration 36, loss = 0.05056478\n",
      "Iteration 37, loss = 0.05073960\n",
      "Iteration 38, loss = 0.05088414\n",
      "Iteration 39, loss = 0.05097109\n",
      "Iteration 40, loss = 0.05098104\n",
      "Iteration 41, loss = 0.05111122\n",
      "Iteration 42, loss = 0.05097819\n",
      "Iteration 43, loss = 0.05042461\n",
      "Iteration 44, loss = 0.05066386\n",
      "Iteration 45, loss = 0.05047643\n",
      "Iteration 46, loss = 0.05035786\n",
      "Iteration 47, loss = 0.05036392\n",
      "Iteration 48, loss = 0.05081369\n",
      "Iteration 49, loss = 0.05080062\n",
      "Iteration 50, loss = 0.05029933\n",
      "Iteration 51, loss = 0.05115691\n",
      "Iteration 52, loss = 0.05223531\n",
      "Iteration 53, loss = 0.05093361\n",
      "Iteration 54, loss = 0.05162785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "m3, tt3, ht3 = mlp_train(train_docs, config=None, use_headers=False)\n",
    "# Save Model\n",
    "save_value('neural_model_100_100_nohead', m3, path=checkpoint_file)\n",
    "save_value('neural_model_tt_100_100_nohead', tt3, path=checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.564531104921077\n",
      "Recall: 0.7621791303973712\n",
      "[[  134   120     0     0     0     0     0     0     0     8     0     2]\n",
      " [   53 25021     1     1     6     8     0     3     0    64     4    68]\n",
      " [    0    33    90     0     0     0     0     0     0     0     0     0]\n",
      " [    0    11     0    81     0     0     0     0     0     0     0     0]\n",
      " [    0    22     1     0    18     0     0     0     0     0     0     0]\n",
      " [    0    40     0     0     0    36     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     1     0     0]\n",
      " [    0    27     0     0     0     0     0    13     0     0     0     1]\n",
      " [    0     1     0     0     0     0     0     0     3     0     0     0]\n",
      " [    2   132     0     0     0     0     0     0     0   204     0     0]\n",
      " [    0     6     0     0     0     0     0     0     0     0    11     4]\n",
      " [    2   287     0     0     0     0     0     2     0     4     0   636]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-186f5d7ec9d4>:35: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = np.where(pred_sum == 0, 0, diagonal/pred_sum)\n"
     ]
    }
   ],
   "source": [
    "# Model Test\n",
    "m3 = load_value('neural_model_100_100_nohead', path=checkpoint_file)\n",
    "tt3 = load_value('neural_model_tt_100_100_nohead', path=checkpoint_file)\n",
    "ht3 = None\n",
    "\n",
    "p, r, cm = model_test(m3, tt3, ht3, list(processed_documents), use_headers=False)\n",
    "print('Precision:', p)\n",
    "print('Recall:', r)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
